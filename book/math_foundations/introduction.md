# Introductie
## Korte omschrijving
In dit OLOD bekijken we de fundamentele bouwstenen van Artificiële Intelligentie (AI) en Machine Learning (ML): lineaire algebra, kansrekenen, statistiek en optimalisatie. Dit doen we met een combinatie van theorie en interactieve voorbeelden in Python, de programmeertaal bij uitstek voor AI toepassingen.  
Lineaire algebra ligt aan de basis van de meeste algoritmes die gebruikt worden voor AI en is belangrijk om te begrijpen wat er zich achter de schermen afspeelt bij deze algoritmes. We bekijken ook een toepassing van lineaire algebra voor beeldcompressie.  
Het gebeurt maar zelden dat we iets met 100% zekerheid kunnen voorspellen, meestal is er een onzekerheid aan verbonden. Dit geldt ook bij
voorspellingen voor AI-toepassingen. Het is dan ook erg belangrijk om de onzekerheid van deze voorspellingen te kunnen inschatten. Kansrekenen leert
ons hoe we met variabelen met onzekerheid kunnen werken en met statistiek kunnen we onzekerheid bepalen.  
Bij de ontwikkeling van AI-algoritmes moet er vaak iets geoptimaliseerd worden. Het doel is vaak om zo optimaal mogelijk de data te beschrijven. We
bekijken hiervoor het gradient descent algoritme dat onder andere aan de basis ligt van deep learning waar veel moderne AI-toepassingen op gebaseerd
zijn.  
De werking van de verschillende bouwstenen wordt geïllustreerd aan de hand van een eenvoudig maar in de praktijk vaak gebruikt ML algoritme: lineaire
regressie.

## OLR-Leerdoelen (lijst)
### Onderzoeken
Beoordeelt de prestaties van machine learning modellen met behulp van intervalschattingen en foutmarges.
### Ontwerpen
Voert het gradient descent algoritme uit voor neurale netwerken met verschillende activerings- en kostenfuncties.  
Bespreekt de eigenschappen van veelgebruikte kansverdelingen in machine learning.  
Past vector- en matrixalgebrabewerkingen toe, zoals scalair product, inverse en determinant.  
Past statistische methoden zoals maximum likelihood estimation (MLE) en maximum a priori (MAP) toe binnen de context van machine learning.  
Beschrijft en kwantificeert de onzekerheid die inherent is aan voorspellingen van modellen voor machine learning.  
### Realiseren
Geeft data weer als vectoren en matrices en identificeert hun eigenschappen met behulp van concepten als singulariteit en lineaire onafhankelijkheid.  
Optimaliseert functies die vaak gebruikt worden bij machine learning analytisch en numeriek met behulp van eigenschappen van afgeleiden.  

## Leerinhoud
- Lineaire algebra
    - Datastructuren: getallen, vectoren, matrices, tensors
    - Algebraïsche operaties
    - Oplossen van een lineair stelsel van vergelijkingen
    - Matrix decompositie en datacompressie
- Kansrekenen
    - Kansvariabelen
    - Conditionele kans (regel van Bayes)
    - Kansverdelingen (Gauss, binomiaal, ...)
- Statistiek
    - P-waarde
    - Betrouwbaarheidsinterval
    - Correlatie vs. causatie
- Calculus en optimalisatie
    - Afgeleide van een functie
    - Het gradient descent algoritme
- Lineaire regressie

## Programma
| Week  | Datum | Onderdeel | Labo |
|-------|-------|----------|----------|
| 1     | 19/09 |   Lineaire algebra #1   |   Vectoren, matrices & tensors   |
| 2     | 26/09 |   Lineaire algebra #2   |   Dot products, speciale tensors & stelsels van lineaire vergelijkingen   |
| 3     | 03/10 |   Gradients #1    |   OLS & gradient descent voor lineaire Regressie   |
| 4     | 10/10 |   Gradients #2   |   Gradient descent voor logistische regressie   |
| 5     | 17/10 |   Gradients #3   |   Backpropagation   |
| 6     | 24/10 |   Probabiliteit #1   |   Kansvariabelen en -verdelingen   |
| Herfstvakantie
| 7     | 07/11 |   Probabiliteit #2   |   Bayesiaanse aanpak van lineaire regressie   |
| 8     | 14/11 |   Probabiliteit #3   |   Causaliteit   |
| 9     | 21/11 |   Datacompressie #1   |   Singular value decomposition   |
| 10    | 28/11 |   Datacompressie #2   |   PCA & t-SNE   |
| 11    | 05/12 |   Datacompressie #3 |   Embeddings & vector similarity   |
| 12    | 12/12 |   Presentaties   |      |
| 13    | 19/12 |   Voorbereiding examen   |      |

## Evaluatie
|   |   |   |
|---|---|---|
| 20% | Permanente Evaluatie | Zie "Opdracht" sectie in de "Afspraken" pagina  |
| 40% | Kennistoets | Gesloten boek, Leerstof: Cursus (online), Eigen notities, Labo' - Zie "Leerstof" sectie in de "Afspraken" pagina |
| 40% | Vaardigheidstoets | Oefeningen in Jupyter notebook, Open boek |
