{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73df8647",
   "metadata": {},
   "source": [
    "# Hypotheses Testen\n",
    "\n",
    "(target-hypothesis-testing)=\n",
    "_**Hypothesis testing**_ is een fundamentele statistische methode om te bepalen of waargenomen data consistent is met een bepaalde kansverdeling. Het biedt een formeel kader om beslissingen te nemen onder onzekerheid en is van cruciaal belang in machine learning voor model evaluatie en validatie.  \n",
    "\n",
    "## ML Toepassingen\n",
    "### Model vergelijking  \n",
    "Wanneer we twee of meer modellen trainen, willen we weten of het ene model _significant_ (of betekenisvol) beter presteert dan het andere, of dat waargenomen verschillen louter het gevolg zijn van toeval in de data. Bijvoorbeeld: is model A met een accuraatheid van 87% werkelijk beter dan model B met 85%, of zou dit verschil verdwijnen bij een andere random train/test split?  \n",
    "  \n",
    "Modelvergelijking is niet alleen van toepassing tijdens de training, maar ook bij:\n",
    "- Featureselectie: Bij het bepalen welke features relevant zijn, testen we of een feature een significante invloed heeft op de voorspelling. Dit helpt bij het vermijden van {ref}`overfitting <target-over-fitting>` door irrelevante features te verwijderen.\n",
    "- A/B Testing: In productie-omgevingen testen we of een nieuwe versie van een model beter presteert dan de bestaande versie. Hypothesis testing vertelt ons wanneer we voldoende bewijs hebben om over te schakelen.\n",
    "- _{ref}`Hyperparameter <target-hyper-parameters>` tuning_: Bij het vergelijken van verschillende hyperparameter configuraties helpt hypothesis testing om te bepalen of waargenomen prestatieverschillen statistisch significant zijn.\n",
    "\n",
    "### Dataset vergelijking\n",
    "(target-drift)=\n",
    "We kunnen testen of trainings- en testdata uit dezelfde distributie komen, of dat er bij incrementele data verschuivingen zijn opgetreden - zogenaamde **data drift** (wat leidt tot gedegradeerde modelprestaties of **model drift**).\n",
    "\n",
    "## Basisconcept\n",
    "\n",
    "Bij hypothesis testing vergelijken we twee veronderstellingen of _hypotheses_:\n",
    "(target-hypothesis)=\n",
    "- **Nulhypothese** ($H_0$): De standaardaanname, meestal de meest conservatieve veronderstelling: de data zijn {u}`consistent` met een veronderstelde kansverdeling.\n",
    "- **Alternatieve hypothese** ($H_1$ of $H_a$): De hypothese die we willen testen, meestal dat de data zijn {u}`niet-consistent` met een veronderstelde kansverdeling.\n",
    "\n",
    "Het doel is om te bepalen of de data voldoende bewijs levert om $H_0$ te verwerpen ten gunste van $H_1$.\n",
    "\n",
    "## p-waarde\n",
    "(target-p-value)=\n",
    "De **p-waarde** is de kans om de waargenomen data en extremere waarden te observeren, gegeven dat de nulhypothese waar is. Een lage p-waarde ($< 0.05$) suggereert dat de data onwaarschijnlijk is onder $H_0$, wat leidt tot verwerping van $H_0$.  \n",
    "  \n",
    "Veronderstel dat onder de nulhypothese de data een standaard normale verdeling $\\mathcal{N}(0, 1)$ volgt. Als we een nieuwe steekproef met gemiddelde $m = 2$ krijgen, dan is de p-waarde $P(m \\geq 2 \\mid H_0)$ - de kans om een waarde van $2$ of extremer te observeren als $H_0$ waar is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28428a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Illustration of p-value concept\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Parameters for H₀: data follows N(0, 1)\n",
    "mu_0 = 0\n",
    "sigma = 1\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = stats.norm.pdf(x, mu_0, sigma)\n",
    "\n",
    "# Observed value\n",
    "observed_value = 2.0\n",
    "\n",
    "# Plot the distribution under H₀\n",
    "ax.plot(x, y, \"b-\", linewidth=2.5, label=\"H₀: N(0, 1)\")\n",
    "ax.axvline(\n",
    "    observed_value,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Observed value = {observed_value}\",\n",
    ")\n",
    "\n",
    "# Shade the p-value region (values ≥ observed)\n",
    "x_extreme = x[x >= observed_value]\n",
    "ax.fill_between(\n",
    "    x_extreme,\n",
    "    stats.norm.pdf(x_extreme, mu_0, sigma),\n",
    "    alpha=0.3,\n",
    "    color=\"red\",\n",
    "    label=\"p-value region\",\n",
    ")\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = 1 - stats.norm.cdf(observed_value, mu_0, sigma)\n",
    "\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(f\"P(X ≥ {observed_value} | H₀) = {p_value:.4f}\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate(\n",
    "    f\"p-value = {p_value:.4f}\",\n",
    "    xy=(observed_value, 0.05),\n",
    "    xytext=(2.5, 0.15),\n",
    "    arrowprops={\"arrowstyle\": \"->\", \"color\": \"red\", \"lw\": 2},\n",
    "    fontsize=12,\n",
    "    color=\"red\",\n",
    "    bbox={\"boxstyle\": \"round,pad=0.5\", \"facecolor\": \"wheat\", \"alpha\": 0.8},\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d6343",
   "metadata": {},
   "source": [
    "\n",
    "## Significantieniveau\n",
    "(target-significance)=\n",
    "Het **significantieniveau** ($\\alpha$) is de drempel voor verwerping van $H_0$. Typisch wordt $\\alpha = 0.05$ gebruikt, wat betekent dat we een $5\\%$ kans op een type I fout accepteren (ten onrechte $H_0$ verwerpen).\n",
    "\n",
    "## Type I en Type II fouten\n",
    "(target-error-types)=\n",
    "Bij hypothesis testing zijn twee soorten fouten mogelijk:\n",
    "\n",
    "- **Type I fout** ($\\alpha$): $H_0$ verwerpen terwijl deze waar is (vals positief)\n",
    "- **Type II fout** ($\\beta$): $H_0$ niet verwerpen terwijl deze vals is (vals negatief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057c380",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualization of Type I and Type II errors\n",
    "mu_0 = 0  # H₀ mean\n",
    "mu_1 = 2  # H₁ mean\n",
    "sigma = 2\n",
    "alpha = 0.05\n",
    "\n",
    "# Critical value for one-tailed test\n",
    "z_critical = stats.norm.ppf(1 - alpha, mu_0, sigma)\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(-6, 8, 1000)\n",
    "y_h0 = stats.norm.pdf(x, mu_0, sigma)\n",
    "y_h1 = stats.norm.pdf(x, mu_1, sigma)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot distributions\n",
    "ax.plot(x, y_h0, \"b-\", linewidth=2, label=\"H₀: μ = 0\")\n",
    "ax.plot(x, y_h1, \"r-\", linewidth=2, label=\"H₁: μ = 2\")\n",
    "\n",
    "# Type I error region (α)\n",
    "x_alpha = x[x >= z_critical]\n",
    "ax.fill_between(\n",
    "    x_alpha,\n",
    "    stats.norm.pdf(x_alpha, mu_0, sigma),\n",
    "    alpha=0.3,\n",
    "    color=\"blue\",\n",
    "    label=f\"Type I error (α = {alpha})\",\n",
    ")\n",
    "\n",
    "# Type II error region (β)\n",
    "x_beta = x[x < z_critical]\n",
    "ax.fill_between(\n",
    "    x_beta, stats.norm.pdf(x_beta, mu_1, sigma), alpha=0.3, color=\"red\", label=\"Type II error (β)\"\n",
    ")\n",
    "\n",
    "# Critical value line\n",
    "ax.axvline(\n",
    "    z_critical,\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Critical value = {z_critical:.2f}\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(\"Type I and Type II Errors in Hypothesis Testing\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2f454",
   "metadata": {},
   "source": [
    "## Eenzijdige vs. Tweezijdige Tests\n",
    "(target-test-side)=\n",
    "Bij hypothesis testing kunnen we kiezen tussen **eenzijdige** en **tweezijdige** tests:\n",
    "\n",
    "- **Tweezijdige test**: We testen of de parameter _verschilt_ van de hypothetische waarde (kan groter of kleiner zijn). Het significantieniveau $\\alpha$ wordt verdeeld over beide staarten van de verdeling ($\\alpha/2$ aan elke kant).\n",
    "  \n",
    "- **Eenzijdige test**: We testen of de parameter _groter_ of _kleiner_ is dan de hypothetische waarde. Het volledige significantieniveau $\\alpha$ wordt aan één kant geplaatst.\n",
    "\n",
    "Bij een tweezijdige test met $\\alpha = 0.05$ gebruiken we kritische waarden van $\\pm 1.96$ voor de standaard normale verdeling, waarbij elk uiteinde 2.5% van de verdeling bevat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e467c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Comparison of one-sided vs two-sided tests\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "mu_0 = 0\n",
    "sigma = 1\n",
    "alpha = 0.05\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = stats.norm.pdf(x, mu_0, sigma)\n",
    "\n",
    "# Left plot: Two-sided test\n",
    "z_critical_two_sided = stats.norm.ppf(1 - alpha / 2)  # 1.96 for α=0.05\n",
    "\n",
    "axes[0].plot(x, y, \"b-\", linewidth=2.5, label=\"N(0, 1)\")\n",
    "axes[0].axvline(\n",
    "    z_critical_two_sided,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"z = ±{z_critical_two_sided:.2f}\",\n",
    ")\n",
    "axes[0].axvline(-z_critical_two_sided, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# Shade both critical regions\n",
    "x_left = x[x <= -z_critical_two_sided]\n",
    "x_right = x[x >= z_critical_two_sided]\n",
    "axes[0].fill_between(\n",
    "    x_left,\n",
    "    stats.norm.pdf(x_left, mu_0, sigma),\n",
    "    alpha=0.3,\n",
    "    color=\"red\",\n",
    "    label=f\"α/2 = {alpha / 2} (each tail)\",\n",
    ")\n",
    "axes[0].fill_between(x_right, stats.norm.pdf(x_right, mu_0, sigma), alpha=0.3, color=\"red\")\n",
    "\n",
    "axes[0].set_xlabel(\"z-value\")\n",
    "axes[0].set_ylabel(\"Probability Density\")\n",
    "axes[0].set_title(f\"Two-Sided Test (α = {alpha})\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "axes[0].annotate(\n",
    "    f\"{alpha / 2:.3f}\",\n",
    "    xy=(-z_critical_two_sided, 0.05),\n",
    "    xytext=(-3, 0.15),\n",
    "    arrowprops={\"arrowstyle\": \"->\", \"color\": \"red\"},\n",
    "    fontsize=11,\n",
    "    color=\"red\",\n",
    ")\n",
    "axes[0].annotate(\n",
    "    f\"{alpha / 2:.3f}\",\n",
    "    xy=(z_critical_two_sided, 0.05),\n",
    "    xytext=(2.5, 0.15),\n",
    "    arrowprops={\"arrowstyle\": \"->\", \"color\": \"red\"},\n",
    "    fontsize=11,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "# Right plot: One-sided test (right tail)\n",
    "z_critical_one_sided = stats.norm.ppf(1 - alpha)  # 1.645 for α=0.05\n",
    "\n",
    "axes[1].plot(x, y, \"b-\", linewidth=2.5, label=\"N(0, 1)\")\n",
    "axes[1].axvline(\n",
    "    z_critical_one_sided,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"z = {z_critical_one_sided:.2f}\",\n",
    ")\n",
    "\n",
    "# Shade right critical region\n",
    "x_right_one = x[x >= z_critical_one_sided]\n",
    "axes[1].fill_between(\n",
    "    x_right_one,\n",
    "    stats.norm.pdf(x_right_one, mu_0, sigma),\n",
    "    alpha=0.3,\n",
    "    color=\"red\",\n",
    "    label=f\"α = {alpha}\",\n",
    ")\n",
    "\n",
    "axes[1].set_xlabel(\"z-value\")\n",
    "axes[1].set_ylabel(\"Probability Density\")\n",
    "axes[1].set_title(f\"One-Sided Test (Right Tail, α = {alpha})\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "axes[1].annotate(\n",
    "    f\"{alpha:.3f}\",\n",
    "    xy=(z_critical_one_sided, 0.05),\n",
    "    xytext=(2.5, 0.15),\n",
    "    arrowprops={\"arrowstyle\": \"->\", \"color\": \"red\"},\n",
    "    fontsize=11,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Critical Values:\")\n",
    "print(f\"Two-sided test (α = {alpha}): z = ±{z_critical_two_sided:.3f}\")\n",
    "print(f\"One-sided test (α = {alpha}): z = {z_critical_one_sided:.3f}\")\n",
    "print(\"\\nNote: For a two-sided test, we split α between both tails.\")\n",
    "print(\"This is why we use ±1.96 for α=0.05 (2.5% in each tail).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32aa580",
   "metadata": {},
   "source": [
    "### Specifieke tests\n",
    "Er bestaan heel veel verschillende types statistische tests naargelang het soort data en de veronderstelde theoretische of populatieverdelingen.\n",
    "\n",
    "#### $z$-test\n",
    "In de {u}`veronderstelling van normaal verdeelde data` kan bijvoorbeeld worden gekeken of het gemiddelde van een steekproef $m$ significant verschilt van een vooropgesteld populatiegemiddelde $\\mu$. Theoretisch is het zo dat als we het verschil $m-\\mu$ standaardiseren met de **standaardfout van het steekproefgemiddelde**, we een standaard normaalverdeling moeten bekomen.\n",
    "\n",
    "$$\n",
    "Z = \\frac{m-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "(target-standard-error-mean)=\n",
    "De standaardfout $SE = \\sigma/\\sqrt{n}$ is de standaardafwijking van de sampling distributie van de gemiddeldes. Ze vertelt ons wat de verwachte spreiding is bij een bepaalde sample grootte over verschillende samples (zie illustratie).  \n",
    "  \n",
    "Voor een bepaalde $z$ kunnen we drie versies van de nulhypothese testen, afhankelijk van het feit of we een **eenzijdige** of **tweezijdige** test van het verschil willen uitvoeren:\n",
    "\n",
    "1. eenzijdige test $H_0: P(Z \\geq z) > \\alpha$\n",
    "2. eenzijdige test $H_0: P(Z \\leq z) > \\alpha$\n",
    "3. tweezijdige test $H_0: min\\{P(Z \\geq z), P(Z \\leq z)\\} > \\alpha/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4fceb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Illustration of standard error decreasing with sample size\n",
    "rng = np.random.default_rng(42)\n",
    "true_mean = 100\n",
    "true_std = 15\n",
    "sample_sizes = [5, 10, 30, 50, 100, 200]\n",
    "n_simulations = 1000\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "theoretical_ses = []\n",
    "empirical_ses = []\n",
    "\n",
    "for idx, n in enumerate(sample_sizes):\n",
    "    # Generate multiple samples and calculate their means\n",
    "    sample_means = []\n",
    "    for _ in range(n_simulations):\n",
    "        sample = rng.normal(true_mean, true_std, n)\n",
    "        sample_means.append(sample.mean())\n",
    "\n",
    "    # Calculate empirical standard error (standard deviation of sample means)\n",
    "    empirical_se = np.std(sample_means, ddof=1)\n",
    "    empirical_ses.append(empirical_se)\n",
    "\n",
    "    # Calculate theoretical standard error\n",
    "    theoretical_se = true_std / np.sqrt(n)\n",
    "    theoretical_ses.append(theoretical_se)\n",
    "\n",
    "    # Plot distribution of sample means\n",
    "    axes[idx].hist(\n",
    "        sample_means,\n",
    "        bins=30,\n",
    "        density=True,\n",
    "        alpha=0.7,\n",
    "        color=\"skyblue\",\n",
    "        edgecolor=\"black\",\n",
    "        label=\"Sample means\",\n",
    "    )\n",
    "\n",
    "    # Overlay theoretical normal distribution\n",
    "    x = np.linspace(min(sample_means), max(sample_means), 100)\n",
    "    y = stats.norm.pdf(x, true_mean, theoretical_se)\n",
    "    axes[idx].plot(x, y, \"r-\", linewidth=2.5, label=\"Theoretical N(μ, SE)\")\n",
    "\n",
    "    axes[idx].axvline(\n",
    "        true_mean, color=\"green\", linestyle=\"--\", linewidth=2, label=f\"True μ = {true_mean}\"\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Sample Mean\")\n",
    "    axes[idx].set_ylabel(\"Density\")\n",
    "    axes[idx].set_title(\n",
    "        f\"n = {n}\\nSE = {empirical_se:.2f} (empirical)\\nSE = {theoretical_se:.2f} (theoretical)\"\n",
    "    )\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary plot: SE vs sample size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(\n",
    "    sample_sizes,\n",
    "    theoretical_ses,\n",
    "    \"b-\",\n",
    "    linewidth=2.5,\n",
    "    marker=\"o\",\n",
    "    markersize=8,\n",
    "    label=\"Theoretical SE = σ/√n\",\n",
    ")\n",
    "ax.plot(\n",
    "    sample_sizes,\n",
    "    empirical_ses,\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    marker=\"s\",\n",
    "    markersize=8,\n",
    "    label=\"Empirical SE (from simulations)\",\n",
    ")\n",
    "ax.set_xlabel(\"Sample Size (n)\")\n",
    "ax.set_ylabel(\"Standard Error\")\n",
    "ax.set_title(\"Standard Error Decreases with Sample Size\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d12fc",
   "metadata": {},
   "source": [
    "#### Student's $t$-test\n",
    "\n",
    "[![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/William_Sealy_Gosset.jpg/250px-William_Sealy_Gosset.jpg?raw=true)](https://en.wikipedia.org/wiki/William_Sealy_Gosset)\n",
    "\n",
    "(target-one-sample-t)=\n",
    "Bij de $z$-test veronderstellen we dat we de standaardfout op het gemiddelde $\\sigma/\\sqrt{n}$ exact kennen op basis van de populatievariantie $\\sigma^2$. Dit is echter in de meeste gevallen niet waar. Er wordt gewerkt met de steekproefvariantie $s^2$ die op zich een schatting is. Vooral bij kleine steekproefgroottes is dit problematisch. Statisticus en chemicus/brouwer William Sealy Gosset (die voor Guinness Breweries werkte en onder het pseudoniem \"Student\" publiceerde), toonde aan dat bij standaardisatie van verschillen tussen gemiddeldes met de sample $SE=s/\\sqrt{n}$, geen standaard normaal verdeling bekomen wordt, maar wel een zogenaamde $t$-distributie met als parameter $n-1$ zogenaamde vrijheidsgraden (_degrees of freedom_; $df$; {cite}`10.2307/2331554`). \n",
    "\n",
    "$$\n",
    "T = \\frac{m - \\mu}{s/\\sqrt{n}} \\sim t(n-1)\n",
    "$$\n",
    "\n",
    "\n",
    "t-distributies hebben volgende kenmerkende eigenschappen:\n",
    "1. Zwaardere staarten: De t-distributie heeft dikkere staarten dan de normale verdeling, wat de extra onzekerheid reflecteert\n",
    "2. Afhankelijk van steekproefgrootte: De vorm wordt bepaald door de vrijheidsgraden ($df = n - 1$)\n",
    "3. Convergentie: Voor grote $n$ (ongeveer $n > 30$) convergeert de t-distributie naar de normale verdeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef891a0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Comparison of t-distribution with normal distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "\n",
    "# Left plot: Different degrees of freedom\n",
    "for df in [1, 3, 10, 30]:\n",
    "    y = stats.t.pdf(x, df)\n",
    "    axes[0].plot(x, y, linewidth=2, label=f\"df = {df}\")\n",
    "\n",
    "# Add normal distribution for comparison\n",
    "y_norm = stats.norm.pdf(x)\n",
    "axes[0].plot(x, y_norm, \"k--\", linewidth=2.5, label=\"Normal distribution\")\n",
    "\n",
    "axes[0].set_xlabel(\"Value\")\n",
    "axes[0].set_ylabel(\"Probability Density\")\n",
    "axes[0].set_title(\"t-Distribution for Different Sample Sizes\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right plot: Zoom on tails to show difference\n",
    "x_tail = np.linspace(2, 4, 500)\n",
    "for df in [3, 10, 30]:\n",
    "    y_tail = stats.t.pdf(x_tail, df)\n",
    "    axes[1].plot(x_tail, y_tail, linewidth=2, label=f\"df = {df}\")\n",
    "\n",
    "y_norm_tail = stats.norm.pdf(x_tail)\n",
    "axes[1].plot(x_tail, y_norm_tail, \"k--\", linewidth=2.5, label=\"Normal distribution\")\n",
    "\n",
    "axes[1].set_xlabel(\"Value\")\n",
    "axes[1].set_ylabel(\"Probability Density\")\n",
    "axes[1].set_title(\"Heavier Tails of t-Distribution (zoomed)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43adafaa",
   "metadata": {},
   "source": [
    "Hieronder illustreren we een t-test voor een voorbeeld waarbij we in een steekproef de lengte van volwassenen hebben gemeten (in cm) en de hypothese willen testen dat het populatiegemiddelde 170 cm bedraagt ($H_0: min\\{P(T \\geq t), P(T \\leq t)\\} > \\alpha/2$, met $\\alpha = 0.05$)  \n",
    "  \n",
    "Dit is een voorbeeld van een zogenaamde **one-sample $t$-test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e18cd",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "true_mean = 172\n",
    "sample_size = 30\n",
    "sample = rng.normal(true_mean, 8, sample_size)\n",
    "\n",
    "# Hypothesis: population mean is 170\n",
    "hypothesized_mean = 170\n",
    "\n",
    "# Perform one-sample t-test\n",
    "t_statistic, p_value = stats.ttest_1samp(sample, hypothesized_mean)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Sample distribution\n",
    "ax1.hist(sample, bins=10, density=True, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n",
    "ax1.axvline(\n",
    "    sample.mean(),\n",
    "    color=\"blue\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Sample mean = {sample.mean():.2f}\",\n",
    ")\n",
    "ax1.axvline(\n",
    "    hypothesized_mean,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"H₀: μ = {hypothesized_mean}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Height (cm)\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"Sample Distribution\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Right plot: t-distribution\n",
    "df = sample_size - 1\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "t_dist = stats.t.pdf(x, df)\n",
    "ax2.plot(x, t_dist, \"k-\", linewidth=2, label=\"t-distribution\")\n",
    "ax2.axvline(\n",
    "    t_statistic, color=\"blue\", linestyle=\"--\", linewidth=2, label=f\"t-statistic = {t_statistic:.2f}\"\n",
    ")\n",
    "\n",
    "# Shade critical regions (two-tailed, α=0.05)\n",
    "alpha = 0.05\n",
    "t_critical = stats.t.ppf(1 - alpha / 2, df)\n",
    "x_left = x[x < -t_critical]\n",
    "x_right = x[x > t_critical]\n",
    "ax2.fill_between(\n",
    "    x_left, stats.t.pdf(x_left, df), alpha=0.3, color=\"red\", label=f\"Critical region (α={alpha})\"\n",
    ")\n",
    "ax2.fill_between(x_right, stats.t.pdf(x_right, df), alpha=0.3, color=\"red\")\n",
    "\n",
    "ax2.set_xlabel(\"t-value\")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax2.set_title(\"t-Distribution with Critical Regions\")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea53a0",
   "metadata": {},
   "source": [
    "#### _Two-Sample_ $t$-test\n",
    "(target-two-sample-t)=\n",
    "De **_two-sample_ $t$-test** vergelijkt de gemiddelden van twee onafhankelijke steekproeven. Er bestaan twee versies afhankelijk van het feit of we gelijke of ongelijke populaties veronderstellen.\n",
    "\n",
    "##### Gelijke varianties\n",
    "\n",
    "Als we veronderstellen dat beide steekproeven dezelfde populatievariantie hebben ($\\sigma_1^2 = \\sigma_2^2$), kunnen we een _gepoolde variantie_ gebruiken:\n",
    "\n",
    "$$\n",
    "s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T = \\frac{m_1 - m_2}{s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim t(n_1 + n_2 - 2)\n",
    "$$\n",
    "\n",
    "waarbij $m_1$ en $m_2$ de steekproefgemiddeldes zijn, $s_1^2$ en $s_2^2$ de steekproefvarianties, en $n_1$ en $n_2$ de steekproefgroottes. \n",
    "\n",
    "##### Ongelijke varianties (Welch's t-test)\n",
    "\n",
    "Deze versie veronderstelt {u}`niet` dat de twee steekproeven dezelfde variantie hebben:\n",
    "\n",
    "$$\n",
    "T = \\frac{m_1 - m_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\sim t(df)\n",
    "$$\n",
    "\n",
    "De vrijheidsgraden $df$ worden berekend via de Welch-Satterthwaite vergelijking:\n",
    "\n",
    "$$\n",
    "df = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\n",
    "$$\n",
    "\n",
    "In de praktijk wordt meestal Welch's t-test gebruikt omdat deze robuuster is en geen assumptie van gelijke varianties vereist.\n",
    "\n",
    "Hieronder illustreren we Welch's t-test om na te gaan of twee groepen van personen uit verschillende populaties van lengtes komen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd9385",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate two samples\n",
    "group1 = rng.normal(175, 7, 40)  # e.g., men\n",
    "group2 = rng.normal(165, 6, 35)  # e.g., women\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_stat, p_val = stats.ttest_ind(group1, group2)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Histograms\n",
    "axes[0].hist(group1, bins=12, alpha=0.6, color=\"steelblue\", edgecolor=\"black\", label=\"Group 1\")\n",
    "axes[0].hist(group2, bins=12, alpha=0.6, color=\"coral\", edgecolor=\"black\", label=\"Group 2\")\n",
    "axes[0].axvline(group1.mean(), color=\"blue\", linestyle=\"--\", linewidth=2)\n",
    "axes[0].axvline(group2.mean(), color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Value\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(\"Distribution Comparison\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plots\n",
    "axes[1].boxplot(\n",
    "    [group1, group2],\n",
    "    labels=[\"Group 1\", \"Group 2\"],\n",
    "    patch_artist=True,\n",
    "    boxprops={\"facecolor\": \"lightblue\", \"alpha\": 0.7},\n",
    "    medianprops={\"color\": \"red\", \"linewidth\": 2},\n",
    ")\n",
    "axes[1].set_ylabel(\"Value\")\n",
    "axes[1].set_title(\"Box Plot Comparison\")\n",
    "axes[1].grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Plot 3: Mean comparison with confidence intervals\n",
    "means = [group1.mean(), group2.mean()]\n",
    "sems = [stats.sem(group1), stats.sem(group2)]\n",
    "ci = [1.96 * sem for sem in sems]  # 95% confidence interval\n",
    "\n",
    "x_pos = [0, 1]\n",
    "axes[2].bar(\n",
    "    x_pos,\n",
    "    means,\n",
    "    yerr=ci,\n",
    "    capsize=10,\n",
    "    alpha=0.7,\n",
    "    color=[\"steelblue\", \"coral\"],\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels([\"Group 1\", \"Group 2\"])\n",
    "axes[2].set_ylabel(\"Mean ± 95% CI\")\n",
    "axes[2].set_title(\"Mean Comparison\")\n",
    "axes[2].grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Group 1 mean: {group1.mean():.2f} ± {group1.std():.2f}\")\n",
    "print(f\"Group 2 mean: {group2.mean():.2f} ± {group2.std():.2f}\")\n",
    "print(f\"\\nt-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "print(f\"\\nConclusion: {'Reject H₀' if p_val < 0.05 else 'Fail to reject H₀'} at α=0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5aa496",
   "metadata": {},
   "source": [
    "#### Binomiaaltest\n",
    "(target-binomial-test)=\n",
    "De **binomiaaltest** wordt gebruikt om te testen of de waargenomen proportie in een steekproef overeenkomt met een veronderstelde populatieproportie. Deze test is geschikt voor binaire uitkomsten (succes/falen, ja/nee, etc.).\n",
    "\n",
    "**Hypothese**: We testen of de proportie successen $p$ gelijk is aan een hypothetische waarde $p_0$:\n",
    "- $H_0: p = p_0$\n",
    "- $H_1: p \\neq p_0$ (tweezijdig) of $H_1: p > p_0$ of $H_1: p < p_0$ (eenzijdig)\n",
    "\n",
    "Onder $H_0$ volgt het aantal successen $k$ in $n$ trials een binomiale verdeling:\n",
    "\n",
    "$$\n",
    "P(X = k \\mid H_0) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}\n",
    "$$\n",
    "\n",
    "De p-waarde wordt berekend als de kans om $k$ of een extremere waarde te observeren onder $H_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b21dc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Binomial test example\n",
    "from scipy.stats import binomtest\n",
    "\n",
    "# Example: Testing if a coin is fair\n",
    "# We flip a coin 100 times and observe 60 heads\n",
    "n_trials = 100\n",
    "n_successes = 60\n",
    "p_null = 0.5  # Fair coin hypothesis\n",
    "\n",
    "# Perform binomial test (two-sided)\n",
    "result = binomtest(n_successes, n_trials, p_null, alternative=\"two-sided\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Binomial distribution under H₀\n",
    "k = np.arange(0, n_trials + 1)\n",
    "pmf = stats.binom.pmf(k, n_trials, p_null)\n",
    "\n",
    "axes[0].bar(k, pmf, alpha=0.7, color=\"skyblue\", edgecolor=\"black\", label=\"P(X=k | H₀)\")\n",
    "axes[0].axvline(\n",
    "    n_successes, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Observed k = {n_successes}\"\n",
    ")\n",
    "axes[0].axvline(\n",
    "    n_trials * p_null,\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Expected k = {n_trials * p_null:.0f}\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Number of Successes\")\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "axes[0].set_title(f\"Binomial Distribution (n={n_trials}, p={p_null})\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Plot 2: Cumulative distribution and p-value regions\n",
    "cdf = stats.binom.cdf(k, n_trials, p_null)\n",
    "axes[1].plot(k, cdf, \"b-\", linewidth=2, label=\"CDF\")\n",
    "axes[1].axvline(\n",
    "    n_successes, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Observed k = {n_successes}\"\n",
    ")\n",
    "axes[1].axhline(\n",
    "    result.pvalue / 2, color=\"orange\", linestyle=\":\", linewidth=2, label=\"p-value/2 threshold\"\n",
    ")\n",
    "axes[1].axhline(1 - result.pvalue / 2, color=\"orange\", linestyle=\":\", linewidth=2)\n",
    "axes[1].set_xlabel(\"Number of Successes\")\n",
    "axes[1].set_ylabel(\"Cumulative Probability\")\n",
    "axes[1].set_title(\"Cumulative Distribution Function\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
