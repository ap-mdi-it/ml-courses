{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b3fb83",
   "metadata": {},
   "source": [
    "# 🔤 Taalverwerking\n",
    "\n",
    "## Natural Language Processing\n",
    "(target-nlp)=\n",
    "_Natural language processing_ (NLP) is, naast _computer vision_, een domein binnen _machine learning_ waar neurale netwerken voor grote doorbraken hebben gezorgd. **NLP richt zich op het automatisch begrijpen, interpreteren en genereren van menselijke taal**. Het is een domein met vele vertakkingen en een lange geschiedenis in _machine learning_. Sinds de introductie van de zogenaamde _Transformer_ architectuur ([](https://doi.org/10.48550/arXiv.1706.03762)) is deze familie van neurale netwerken de dominante modelbenadering geworden in dit domein[^vision_transformer]. Deze architectuur gaf aanleiding tot de huidige golf van _Large Language foundation_ modellen.  \n",
    "\n",
    "[^vision_transformer]: en andere domeinen zoals beeldherkenning -en generatie.\n",
    "\n",
    "Net zoals bij beelddata hebben we hier te maken met {ref}`ongestructureerde data <target-unstructured-data>` (Tekstdocumenten: Word-bestanden, PDF’s, enz.).  \n",
    "\n",
    ":::{important} Eigenschappen\n",
    "- **Sequentiële patronen**: We zijn op zoek naar betekenisvolle patronen binnen een strikte volgorde van karakters, woorden, zinnen, paragrafen enz.\n",
    "- **Variabele lengte**: Teksten kunnen sterk variëren in lengte, van enkele woorden tot lange documenten\n",
    "- **Complexe afhankelijkheidsstructuur**: Betekenis hangt af van context, woordvolgorde en taalkundige nuances. Die afhankelijkheid speelt zich af zowel in de onmiddellijke nabijheid (bv. dezelfde zin) als op soms zeer lange afstanden (bv. conversaties) \n",
    "- **Discrete inputs**: Woorden moeten vertaald worden naar numerieke features (zie: _tokenization_)\n",
    "- **Veel trainingsdata nodig**\n",
    "- **_Supervised_, _semi-supervised_ of _self-supervised learning_**\n",
    "- **Vrijwel altijd op basis van neurale netwerk modellen**\n",
    ":::\n",
    "\n",
    "Alles draait om de **automatische extractie van semantische patronen en taalkundige structuren**. Vroege NLP toepassingen waren vaak gebaseerd op combinaties van _rule based_ en statistische verwerking. Tot aan de doorbraak van _Transformer_ modellen, deden neurale netwerken eerst hun intrede bij NLP via _recurrent neural networks_ ([_RNN_](https://en.wikipedia.org/wiki/Recurrent_neural_network)), gevolgd door _long short-term memory_ ([_LSTM_](https://en.wikipedia.org/wiki/Long_short-term_memory)) en _gated recurrent unit_ ([_GRU_](https://en.wikipedia.org/wiki/Gated_recurrent_unit)) neurale netwerken.\n",
    "\n",
    ":::{note} Rule-based systemen\n",
    ":icon: false\n",
    ":class: dropdown\n",
    "Een voorbeeld van een klassieke _rule-based_ benadering is _Part-of-Speech (POS) tagging_ met handgeschreven regels.\n",
    "\n",
    "[![](https://img.shields.io/badge/Wikipedia-000?logo=wikipedia&logoColor=fff&style=flat)](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "\n",
    "_Part-of-Speech tagging_ is een klassieke NLP taak waarbij elk woord in een zin een grammaticale categorie krijgt toegewezen (zelfstandig naamwoord, werkwoord, bijvoeglijk naamwoord, etc.).\n",
    "\n",
    "Vroege systemen gebruikten regels voor het Engels zoals:\n",
    "- Woorden die eindigen op \"-ing\" zijn waarschijnlijk werkwoorden\n",
    "- Woorden die volgen op \"the\" zijn waarschijnlijk zelfstandige naamwoorden\n",
    "- Woorden in hoofdletters aan het begin van een zin zijn waarschijnlijk eigennamen\n",
    "- Woorden die eindigen op \"-ly\" zijn waarschijnlijk bijwoorden\n",
    "- enz.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3e8be",
   "metadata": {},
   "source": [
    "## _Transformers_\n",
    "(target-transformers)=\n",
    "Moderne NLP gebeurt bijna uitsluitend op basis van **_Transformers_**. Het is een specifieke neurale netwerkarchitectuur die in 2017 werd geïntroduceerd. Hun succes vloeit voort uit het feit dat ze **woorden in hun context begrijpen via zogenaamde _self-attention_**. Dit laatste is een heel krachtig mechanisme om met de complexe afhankelijkheidsstructuren bij taalverwerking om te gaan.\n",
    "\n",
    "De oorspronkelijke architectuur ziet er als volgt uit (figuur uit [](https://doi.org/10.48550/arXiv.1706.03762)):\n",
    "\n",
    "[![](../../../img/attention1.png)](https://arxiv.org/pdf/1706.03762)\n",
    "\n",
    "### _Self-attention_\n",
    "Door dit mechanisme kan een model bij het verwerken van een bepaald woord naar _alle_ andere woorden in de sequentie \"kijken\" en \"beslissen\" welke het belangrijkst zijn voor het begrip van dat specifieke woord. Voor mensen is dit evident, maar het is verre van evident gebleken om dit in _machine learning_ na te bootsen[^earlier].\n",
    "\n",
    "[^earlier]: Eerdere benaderingen zoals _RNN_, _LSTM_ en _GRU_ neurale netwerken schoten hier telkens in te kort.\n",
    "  \n",
    "Neem bijvoorbeeld de zin:\n",
    "> _Het **_vliegtuig_** kon niet vertrekken omdat **_het_** een technisch probleem had_.  \n",
    "\n",
    "Als mens begrijpen we onmiddellijk dat **_het_** hier verwijst naar **_vliegtuig_**, maar hoe lost _self-attention_ dit op?  \n",
    "Om dit te begrijpen, kunnen we denken aan een bibliotheek waarin we ($\\approx$ het model) op zoek zijn naar informatie over een bepaald onderwerp ($\\approx$ token). Die informatie staat verspreid over verschillende boeken ($\\approx$ andere tokens in de sequentie). _Self-attention_ voert een dergelijke zoektocht uit op basis van drie belangrijke componenten die moeten worden geleerd tijdens de training.\n",
    "1. **_Query_ (Q)**: De _zoekvraag_; dit is bijvoorbeeld (een abstracte vorm van): \"Ik ben een voornaamwoord... naar wie verwijs ik?\"\n",
    "\n",
    "2. **_Key_ (K)**: De eigenschappen die een woord/token _adverteert_; bijvoorbeeld: \"Ik ben een zelfstandig naamwoord, een mogelijk onderwerp...\". Dit kunnen we vergelijken met de labels op de rug van de boeken in de bibliotheek.\n",
    "\n",
    "3. **_Value_ (V)**: De echte _betekenis_ van het woord in deze context. Denk aan de daadwerkelijke inhoud van een boek.  \n",
    "  \n",
    "De verwerking gaat dan als volgt:\n",
    "1. Voor ieder token worden de Q, K en V vectoren berekend\n",
    "2. Ieder token vergelijkt zijn _Query_ met de _Keys_ van de andere woorden wat resulteert in een genormaliseerde score. Bijvoorbeeld:\n",
    "    - `Q_het` • `K_Het` = 0.0 (niet relevant)\n",
    "    - `Q_het` • `K_vliegtuig` = 0.88 (zeer relevant)\n",
    "    - ...\n",
    "    - `Q_het` • `K_probleem` = 0.65\n",
    "3. Aan de hand van die score weegt ieder woord de _Value_ vectoren (dus de eigenlijke betekenis) van andere woorden.  \n",
    "  \n",
    "In de architectuur zitten niet één, maar verschillende (_multi_) _self-attention heads_. Deze worden elk onafhankelijk van elkaar getraind en toegepast. Zo kan bij eenzelfde token tegelijk rekening gehouden worden met verschillende taalkundige eigenschappen (zie onderstaande figuur uit [](https://doi.org/10.48550/arXiv.1706.03762)).\n",
    "\n",
    "[![](../../../img/attention2.png)](https://arxiv.org/pdf/1706.03762)\n",
    "\n",
    "De output van de verschillende _heads_ wordt samengevoegd in de output naar volgende lagen. Door hun diepe structuur (zie `N x` bij de blokken in de figuur) kunnen transformers ook veel grotere verbanden leren leggen, dus niet enkel op het niveau van individuele zinnen, maar ook uiteenzettingen, conversaties, enz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c38c2",
   "metadata": {},
   "source": [
    "### _Encoder-Decoder_\n",
    "De oorspronkelijke _Transformer_ architectuur ([](https://doi.org/10.48550/arXiv.1706.03762)) bestaat uit twee hoofdstructuren die samenwerken bij _sequence-to-sequence_ taken zoals _machine translation_:\n",
    "\n",
    "#### Encoder\n",
    "(links in de figuur)\n",
    "- Verwerkt de  volledige input sequentie (bv. een zin in het Frans)\n",
    "- Bestaat uit verschillende identieke lagen (6 in de originele architectuur)\n",
    "- Elke laag bevat _multi-head self-attention_ waarbij tokens naar alle andere tokens in de sequentie kunnen \"kijken\"\n",
    "- _Output_: Een _dense_ contextuele representatie van de _input_\n",
    "\n",
    "#### Decoder\n",
    "(rechts in de figuur)  \n",
    "\n",
    ":::{note} 🤗 Autoregressie\n",
    ":icon: false\n",
    ":class: dropdown\n",
    "[![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AutoregressionSchema.gif)](https://huggingface.co/learn/agents-course/en/unit1/what-are-llms)\n",
    ":::\n",
    "\n",
    "- Genereert de output sequentie _token voor token_ (bv. de vertaling in het Engels)\n",
    "- Bij elk nieuw gegenereerd token wordt dit terug als input in de decoder aangeboden (dit heet algemeen _autoregressie_)\n",
    "- Bestaat ook uit verschillende identieke lagen (6 in de originele architectuur)\n",
    "- Voor de decoder input wordt met _masked multi-head self-attention_ gewerkt: in tegenstelling tot de encoder, gebeurt de verwerking van tokens enkel door te kijken naar voorgaande tokens in de sequentie\n",
    "\n",
    ":::{note} 🤗 Decoding\n",
    ":icon: false\n",
    ":class: dropdown\n",
    "<iframe src=\"https://agents-course-decoding-visualizer.hf.space\" frameborder=\"0\" width=\"850\" height=\"450\"></iframe>\n",
    ":::\n",
    "\n",
    ":::{note} Moderne varianten\n",
    "Hoewel de originele architectuur een _encoder-decoder_ was, zijn er inmiddels verschillende varianten:\n",
    "- **_Encoder-only_** (BERT): Alleen voor begrip/classificatie taken\n",
    "- **_Decoder-only_** (GPT): Voor tekst generatie\n",
    "- **_Encoder-decoder_** (T5, BART): Voor _sequence-to-sequence_ taken\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1a639",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "(target-transformer-parameters)=\n",
    "De parameters van _transformer_ modellen zijn enorm talrijk en complex. De belangrijkste zijn:\n",
    "- _Token embeddings_: Ieder token (ID) is gelinkt aan een _dense_ vector in een _embedding_ matrix. Die matrix wordt getraind.\n",
    "- _Positional embeddings_: Een van de voordelen van _Transformers_ is dat de verwerking van tokens in parallel kan gebeuren. In tegenstelling tot architecturen die berusten op recurrente verbindingen of convoluties, moet daarom expliciete informatie over de relatieve positie van een token in de sequentie meegegeven worden. Dit kan ook via getrainde parameters gebeuren[^pos_emb]. \n",
    "\n",
    "[^pos_emb]: In [](https://doi.org/10.48550/arXiv.1706.03762) werden een vaste sinusoïde encodering gebruikt. \n",
    "  \n",
    "Voor elke _attention head_ in elke _layer_:\n",
    "- _Query (Q) matrix_: Transformeert _input_ naar _query vector_\n",
    "- _Key (K) matrix_: Transformeert _input_ naar _key vector_  \n",
    "- _Value (V) matrix_: Transformeert _input_ naar _value vector_\n",
    "- _Output projection_: Combineert _outputs_ van _multiple heads_\n",
    "\n",
    ":::{important} _Large Language Models_ (_LLMs_)\n",
    "Bedrijven zoals OpenAI zijn de transformer architectuur de laatste jaren enorm gaan opschalen om tot de verbluffende performantie te komen die we vandaag kennen:\n",
    "- **BERT-base**: ~110 miljoen parameters\n",
    "- **GPT-2**: ~1.5 miljard parameters  \n",
    "- **GPT-3**: ~175 miljard parameters\n",
    "- **GPT-4**: Geschat >1 triljoen parameters\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118d193",
   "metadata": {},
   "source": [
    "### Features\n",
    "(target-tokenization)=\n",
    "We weten intussen dat om machine learning toe te passen alle data naar numerieke waarden omgezet moet worden. In NLP neemt dit proces een speciale vorm aan: **_tokenization_**. \n",
    "\n",
    "**_Tokenization_ is de eerste en cruciale stap in NLP waarbij ruwe tekst wordt opgedeeld in kleinere eenheden genaamd _tokens_ - ongeacht het model type**. Deze tokens vormen de elementaire deeltjes die door machine learning modellen verwerkt worden.\n",
    "\n",
    "Er bestaan weliswaar verschillende tokenization strategieën, elk met hun eigen voor- en nadelen:\n",
    "\n",
    "#### _Word-level_\n",
    "Dit is de eenvoudigste benadering waarbij tekst wordt opgesplitst op basis van spaties en interpunctie. Elk uniek woord krijgt een eigen ID. \n",
    "```\n",
    "All models are wrong  \n",
    "| All | models | are | wrong |  \n",
    "[2460, 4211, 527, 5076]\n",
    "``` \n",
    "Voordelen:  \n",
    "- Intuïtief en gemakkelijk te begrijpen\n",
    "- Behoudt woordgrenzen  \n",
    "  \n",
    "Nadelen:  \n",
    "- Groot vocabularium (honderdduizenden tokens)\n",
    "- Geen representatie voor _out-of-vocabulary_ woorden\n",
    "- Morfologische varianten worden als volledig verschillende tokens behandeld  \n",
    "  \n",
    "#### _Character-level_\n",
    "Hier wordt ieder individueel karakter een token.\n",
    "```\n",
    "All models are wrong  \n",
    "| A | l | l | _ | m | o | d | e | l | s | _ | a | r | e | _ | w | r | o | n | g |  \n",
    "[65, 108, 108, 32, 109, 111, 100, 101, 108, 115, 32, 97, 114, 101, 32, 119, 114, 111, 110, 103]\n",
    "``` \n",
    "Voordelen:  \n",
    "- Klein vocabularium\n",
    "- Geen _out-of-vocabulary_ problemen\n",
    "- Kan morfologische patronen leren  \n",
    "  \n",
    "Nadelen:  \n",
    "- Langere sequenties\n",
    "- Moeilijker om semantische informatie te leren  \n",
    "\n",
    "#### _Sub-word_\n",
    "Hierbij splitst men woorden op in kleinere, betekenisvolle eenheden. Voorbeelden zijn [_Byte-Pair Encoding_ (BPE)](https://en.wikipedia.org/wiki/Byte-pair_encoding) en [_WordPiece_](https://huggingface.co/learn/llm-course/en/chapter6/6).  \n",
    "```\n",
    "All unmeaningfulish models are wrong\n",
    "| All | un | mean | ing | ful | ish | models | are | wrong\n",
    "[1398, 8362, 3263, 7192, 2118, 2365, 2944, 1132, 2488]\n",
    "```\n",
    "Voordelen:\n",
    "- Balans tussen vocabularium grootte en lengte van sequenties\n",
    "- Kan omgaan met _out-of-vocabulary_ woorden\n",
    "- Deelt informatie tussen morfologisch gerelateerde woorden\n",
    "- Meest gebruikt in moderne _transformers_ (BERT, GPT, T5, enz.)  \n",
    "  \n",
    "Nadelen:  \n",
    "- Minder intuïtief\n",
    "- Vereist voorafgaande training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21bce6",
   "metadata": {},
   "source": [
    ":::{note} 🤗 Tokenizer playground\n",
    ":icon: false\n",
    ":class: dropdown\n",
    "<iframe\n",
    "    src=\"https://agents-course-the-tokenizer-playground.static.hf.space\"\n",
    "    frameborder=\"0\"\n",
    "    width=\"850\"\n",
    "    height=\"450\"\n",
    "></iframe>\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e15d0",
   "metadata": {},
   "source": [
    "### Leeralgoritme\n",
    "\n",
    "_Transformers_ worden zoals andere neurale netwerken getraind met varianten van _stochastic gradient descent_ en gebruiken _backpropagation_ om gradiënten te berekenen door het hele netwerk. _Loss_ functies nemen verschillende vormen aan per taak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a14c8e",
   "metadata": {},
   "source": [
    "### Taken\n",
    "\n",
    "_Transformers_ worden voor een breed scala aan NLP taken ingezet.\n",
    "\n",
    "#### Klassificatie\n",
    "Zowel op het niveau van teksten als individuele tokens bestaan verschillende varianten, bijvoorbeeld:\n",
    "- _Sentiment_ analyse: Bepalen of een tekst positief, negatief of neutraal is\n",
    "- _Topic_ classificatie: Indelen in thematische categorieën\n",
    "- _Spam_ detectie: Onderscheiden van ongewenste berichten\n",
    "- _Named Entity Recognition (NER)_: bv. Is dit een eigennaam?\n",
    "\n",
    ":::{note} 🌍\n",
    ":icon: false\n",
    ":class: dropdown\n",
    "Voorbeelden: \n",
    "- [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)\n",
    "- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
    ":::\n",
    "\n",
    "#### Automatische vertaling\n",
    "Het model vertaalt tekst van de ene taal naar de andere. Dit gebeurt aan de hand van een _encoder-decoder_ architectuur.\n",
    "\n",
    ":::{note} 🌍\n",
    ":icon: false\n",
    ":class: dropdown\n",
    "Voorbeelden: \n",
    "- [M2M-100](https://huggingface.co/docs/transformers/model_doc/m2m_100)\n",
    "- [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)\n",
    ":::\n",
    "\n",
    "#### Vragen beantwoorden\n",
    "Binnen deze taak is het _chatten_ vandaag de meest gekende toepassing. Er vallen specifieke taken onder zoals het opvolgen van instructies, genereren van redeneringen.\n",
    "Hiervoor worden verschillende (complexe) subtaken opgezet tijdens het trainen.\n",
    "\n",
    ":::{note} 🌍\n",
    ":icon: false\n",
    ":class: dropdown\n",
    "Voorbeelden: \n",
    "- [GPT-4](https://openai.com/gpt-4)\n",
    "- [Llama](https://llama.meta.com/)\n",
    "- [Mistral](https://mistral.ai/)\n",
    ":::\n",
    "\n",
    "#### _Representation learning_\n",
    "_Encoder-only_ modellen genereren _embeddings_ die nuttig zijn voor diverse _downstream_ taken, zoals _similarity search_ en _few-/zero-shot_ klassificatie\n",
    "\n",
    ":::{note} 🌍\n",
    ":icon: false\n",
    ":class: dropdown\n",
    "Voorbeelden: \n",
    "- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "- [E5](https://huggingface.co/intfloat/e5-large-v2)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b32fe",
   "metadata": {},
   "source": [
    "### Ervaring\n",
    "Moderne _transformer_ architecturen (LLMs) worden doorgaans in twee fases getraind:\n",
    "1. _pre-training_\n",
    "2. _fine-tuning_  \n",
    "  \n",
    "#### _Pre-training_\n",
    "Tijdens _pre-training_ gebeurt training via **_self-supervised learning_**. Hierbij bestaan er verschillende varianten. De meest eenvoudige zijn:\n",
    "\n",
    "- _Causal language modeling_: Het model voorspelt het volgende token in een sequentie van tokens en krijgt daarop rechtstreeks feedback.\n",
    "```\n",
    "The capital of France is _ | target: Paris\n",
    "```\n",
    "- _Masked language modeling_: Het model moet leren om gedeeltelijk gemaskeerde input sequenties te reconstrueren.\n",
    "```\n",
    "The _ of France is _ | targets: capital, Paris\n",
    "```\n",
    "Het is de bedoeling dat het model een generiek begrip opbouwt van een (of meerdere) taal/talen. Dit vraagt enorme volumes aan data en rekenkracht.\n",
    "  \n",
    "#### _Fine-tuning_\n",
    "Na de _pre-training_ worden modellen gericht naar specifieke taken en objectieven. Hier wordt dan gebruik gemaakt van **_supervised_** _training regimes_.  \n",
    "In de context van moderne chatbots en assistenten worden speciale _fine-tuning_ technieken ingezet, waaronder:\n",
    "- _Instruction tuning_: het model wordt _supervised_ getraind op diverse taken geformuleerd als instructies:\n",
    "```\n",
    "\"Vertaal naar Engels: Hallo wereld\"\n",
    "\"Vat samen: [lange tekst]\"\n",
    "\"Beantwoord de vraag: Wat is ML?\"\n",
    "```\n",
    "- _Reinforcement Learning from Human Feedback (RLHF)_:\n",
    "Tijdens de _pre-training_ fase leren LLMs om taal te produceren die quasi niet te onderscheiden is van natuurlijke taal. Ze zijn daarom niet per se goed in het geven van nuttige/correcte/veilige antwoorden. _RLHF_ werd geïntroduceerd door OpenAI en Google DeepMind ([](  \n",
    "https://doi.org/10.48550/arXiv.1706.03741)) in de aanloop naar [ChatGPT](https://openai.com/index/chatgpt/). Ze lieten op grote schaal een _pre-trained_ model verschillende antwoorden genereren voor bepaalde vragen en vroegen echte mensen de antwoorden te scoren op wenselijkheid.\n",
    "```\n",
    "Question: \"How do I make pasta?\"\n",
    "\n",
    "Response A: \"Boil water, add salt, cook pasta for 8-10 minutes, drain and serve.\"\n",
    "Response B: \"Pasta pasta pasta delicious yummy food.\"\n",
    "\n",
    "Score Response A > Score Response B\n",
    "```\n",
    "Met die informatie werd een apart (_reward_) model getraind om voor iedere output van het taalmodel de \"menselijke wenselijkheid\" te voorspellen. Daarna werd dit _reward_ model gebruikt om het taalmodel te _fine-tunen_ via _reinforcement learning_ richting meer nuttige/correcte/veilige output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7ded6",
   "metadata": {},
   "source": [
    "### Evaluatie\n",
    "\n",
    "Naast klassieke metrieken (bv. voor klassificatie), bestaan er algemeen in de context van NLP veel specifieke varianten zoals: \n",
    "\n",
    "- [Word error rate](https://en.wikipedia.org/wiki/Word_error_rate)\n",
    "- [BLEU](https://en.wikipedia.org/wiki/BLEU) (Bilingual Evaluation Understudy)\n",
    "- [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- [METEOR](https://en.wikipedia.org/wiki/METEOR)\n",
    "- [BERTScore](https://arxiv.org/abs/1904.09675)\n",
    "- [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "- [Seqeval](https://github.com/chakki-works/seqeval)\n",
    "- ...\n",
    "\n",
    "Voor veel NLP taken is automatische evaluatie echter ontoereikend en wordt er geïnvesteerd in menselijke scoring:\n",
    "- _Fluency_: Is de gegenereerde tekst vloeiend en grammaticaal correct?\n",
    "- _Coherence_: Is de tekst logisch en samenhangend?\n",
    "- _Relevance_: Is de output relevant voor de input?\n",
    "- _Factuality_: Zijn de feiten correct?\n",
    "- _Safety_: Is de output veilig en ethisch verantwoord?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33632b",
   "metadata": {},
   "source": [
    "### Voordelen\n",
    "- _State-of-the-art_ performantie: _Transformers_ behalen de beste resultaten op vrijwel alle NLP _benchmarks_\n",
    "- _Transfer learning_: _Pre-trained_ modellen zijn herbruikbaar voor vele taken\n",
    "- Parallellisatie: Snelle training op moderne hardware (GPUs/TPUs)\n",
    "- Lange-termijn afhankelijkheden: Kunnen context over lange afstanden modelleren zonder de beperkingen van sequentiële verwerking\n",
    "- _Multi-task learning_: Eén model kan meerdere taken aan\n",
    "- _Multi-lingual_: Moderne modellen werken vaak over talen heen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a2642",
   "metadata": {},
   "source": [
    "### Nadelen\n",
    "(target-hallucinations)=\n",
    "- _Training_: Zeer intensief (duurtijd in de orde van maanden)\n",
    "- _Inference_: Groot geheugen en rekenkracht nodig voor grote modellen; trage responsen\n",
    "- Energie & water verbruik: Aanzienlijke milieu-impact van _training_ en _deployment_\n",
    "- Kosten: _Hardware_ (GPUs) en _cloud computing_ zeer kostelijk\n",
    "- Interpreteerbaarheid: Complexe, ontransparante architectuur\n",
    "- _Toxiciteit_: Kunnen ongepaste of schadelijke inhoud genereren - moeilijk om 100% te controleren\n",
    "- **_Hallucinaties_**: Kunnen **overtuigend feitelijk incorrecte _output_ genereren**\n",
    "- {ref}`Over fitting <target-over-fitting>`: Kunnen trainingsdata memoriseren en lekken\n",
    "- _Training window_: Om modellen als kennisbronnen te gebruiken moeten we rekening houden met de _training data cutoff_ datum\n",
    "- _Context window_: Een _context window_ is het maximale aantal tokens dat het model tegelijk kan \"zien\". Dit kan 4.000 tokens zijn, 8.000, of zelfs meer dan een miljoen in de nieuwste modellen. Informatie buiten dit venster kan niet in rekening worden gebracht en moet, indien nodig, worden aangereikt via een database van _embeddings_[^rag].\n",
    "\n",
    "[^rag]: zoals in het zogenaamde _Retrieval Augmented Generation_ (_RAG_) framework"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
