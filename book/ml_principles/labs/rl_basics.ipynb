{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5682fe",
   "metadata": {},
   "source": [
    "# Basis _reinforcement learning_\n",
    "\n",
    "## CartPole\n",
    "\n",
    "CartPole is een klassiek controle probleem waarbij een staaf rechtop moet blijven op een kar die heen en weer kan bewegen.\n",
    "\n",
    "[![CartPole](https://gymnasium.farama.org/_images/cart_pole.gif)](https://gymnasium.farama.org/)\n",
    "\n",
    "### Het Probleem\n",
    "- **State**: 4 continue waarden (positie kar, snelheid kar, hoek staaf, hoeksnelheid staaf)\n",
    "- **Actions**: 2 discrete acties (duw naar links of rechts)\n",
    "- **Reward**: +1 voor elke tijdstap waarbij de staaf rechtop blijft\n",
    "- **Doel**: Hou de staaf zo lang mogelijk rechtop (max 500 tijdstappen)\n",
    "\n",
    "### Setup\n",
    "We gebruiken:\n",
    "- [Gymnasium](https://gymnasium.farama.org/index.html): Een framework voor RL omgevingen (oorspronkelijk van OpenAI)\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/#): Kwalitatieve PyTorch implementaties van RL algoritmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df8212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "# from matplotlib import animation\n",
    "\n",
    "\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daff1671",
   "metadata": {},
   "source": [
    "### De Omgeving Verkennen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset environment to get initial state\n",
    "state, info = env.reset(seed=42)\n",
    "\n",
    "print(\"=== CartPole Environment ===\")\n",
    "print(f\"State space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"\\nInitial state: {state}\")\n",
    "print(\"\\nState components:\")\n",
    "print(f\"  [0] Cart Position: {state[0]:.3f}\")\n",
    "print(f\"  [1] Cart Velocity: {state[1]:.3f}\")\n",
    "print(f\"  [2] Pole Angle: {state[2]:.3f}\")\n",
    "print(f\"  [3] Pole Angular Velocity: {state[3]:.3f}\")\n",
    "print(\"\\nPossible actions:\")\n",
    "print(\"  0: Push cart to the LEFT\")\n",
    "print(\"  1: Push cart to the RIGHT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511d9c0",
   "metadata": {},
   "source": [
    "### Random baseline agent\n",
    "\n",
    "Voordat we een intelligent model trainen, kijken we eerst hoe een **random agent** (die willekeurige acties neemt) presteert. Dit geeft ons een baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test random agent\n",
    "def evaluate_random_agent(env, n_episodes=10, seed=42):\n",
    "    \"\"\"\n",
    "    Evaluate a random agent that takes random actions.\n",
    "\n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        n_episodes: Number of episodes to run\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        List of episode rewards\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, info = env.reset(seed=seed + episode)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (done or truncated):\n",
    "            # Random action\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Evaluate random agent\n",
    "random_rewards = evaluate_random_agent(env, n_episodes=100)\n",
    "\n",
    "print(\"=== Random Agent Performance ===\")\n",
    "print(f\"Average reward: {np.mean(random_rewards):.2f} Â± {np.std(random_rewards):.2f}\")\n",
    "print(f\"Min reward: {np.min(random_rewards):.2f}\")\n",
    "print(f\"Max reward: {np.max(random_rewards):.2f}\")\n",
    "\n",
    "px.histogram(random_rewards, nbins=20, title=\"Random Agent: Reward Distribution\").add_vline(\n",
    "    x=np.mean(random_rewards),\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=f\"Mean: {np.mean(random_rewards):.1f}\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    y=random_rewards,\n",
    "    title=\"Random Agent: Reward per Episode\",\n",
    "    labels={\"x\": \"Episode\", \"y\": \"Reward\"},\n",
    ").add_hline(\n",
    "    y=np.mean(random_rewards),\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=f\"Mean: {np.mean(random_rewards):.1f}\",\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71373c",
   "metadata": {},
   "source": [
    "### Training met Deep Q-Network (DQN)\n",
    "\n",
    "Nu gaan we een **Deep Q-Network (DQN)** trainen om een intelligente policy te leren. DQN is een value-based methode die een neural network gebruikt om de optimale $Q$-functie $Q^*(s,a)$ te benaderen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb471a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh environment for training\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Create DQN model with better hyperparameters\n",
    "# The neural network will learn Q(s,a) for each state-action pair\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",  # Multi-Layer Perceptron policy network\n",
    "    env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50000,\n",
    "    learning_starts=1000,  # Start learning after more experiences\n",
    "    batch_size=64,  # Larger batch size for more stable learning\n",
    "    tau=1.0,\n",
    "    gamma=0.99,  # Discount factor\n",
    "    train_freq=4,\n",
    "    target_update_interval=250,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.02,  # Lower final exploration\n",
    "    # verbose=1,  # Show training progress\n",
    "    tensorboard_log=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6878157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent for longer\n",
    "model.learn(total_timesteps=50000, progress_bar=True)\n",
    "\n",
    "# Evaluate the trained model (wrap env with Monitor to avoid warning)\n",
    "eval_env = Monitor(gym.make(\"CartPole-v1\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100, deterministic=True)\n",
    "eval_env.close()\n",
    "\n",
    "print(f\"  Random Agent: {np.mean(random_rewards):.2f} Â± {np.std(random_rewards):.2f}\")\n",
    "print(f\"  Trained DQN:  {mean_reward:.2f} Â± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9030f",
   "metadata": {},
   "source": [
    "### Visualisatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def create_animation(frames, interval=50):\n",
    "    \"\"\"\n",
    "    Create an animation from frames.\n",
    "\n",
    "    Args:\n",
    "        frames: List of RGB arrays\n",
    "        interval: Delay between frames in milliseconds\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        matplotlib animation object\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Display first frame\n",
    "    img = ax.imshow(frames[0])\n",
    "\n",
    "    def animate(frame_idx):\n",
    "        img.set_array(frames[frame_idx])\n",
    "        ax.set_title(f\"Step {frame_idx}/{len(frames) - 1}\", fontsize=14)\n",
    "        return [img]\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=interval, blit=True)\n",
    "\n",
    "    plt.close()  # Don't show the static figure\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trained agent\n",
    "def visualize_agent(model, env, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Run agent in environment and collect frames for visualization.\n",
    "\n",
    "    Args:\n",
    "        model: Trained RL model\n",
    "        env: Gymnasium environment\n",
    "        n_steps: Maximum number of steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        frames, rewards, actions\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    rewards_list = []\n",
    "    actions_list = []\n",
    "\n",
    "    state, info = env.reset(seed=42)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Get action from trained policy (deterministic)\n",
    "        action, _states = model.predict(state, deterministic=True)\n",
    "        actions_list.append(int(action))\n",
    "\n",
    "        # Take action in environment\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        rewards_list.append(reward)\n",
    "        frames.append(env.render())\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return frames, rewards_list, actions_list\n",
    "\n",
    "\n",
    "# Create environment with rendering\n",
    "env_render = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "frames, rewards_list, actions_list = visualize_agent(model, env_render, n_steps=1000)\n",
    "env_render.close()\n",
    "\n",
    "print(f\"\\nEpisode lasted {len(rewards_list)} steps\")\n",
    "print(f\"Total reward: {sum(rewards_list):.0f}\")\n",
    "print(f\"Action distribution: LEFT={actions_list.count(0)}, RIGHT={actions_list.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b11d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation of the trained CartPole agent\n",
    "print(\"Creating animation...\")\n",
    "anim = create_animation(frames, interval=50)\n",
    "\n",
    "# Display the animation\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bf3ec",
   "metadata": {},
   "source": [
    "### Analyse\n",
    "\n",
    "DQN leert een **Q-function** $Q(s,a)$ die voor elke state-action combinatie voorspelt wat de **verwachte cumulatieve reward** (_return_) is.\n",
    "\n",
    "Bij een greedy policy, kiest de agent altijd actie met de hoogste Q-waarde:\n",
    "$$\\pi(s) = \\arg\\max_a Q(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ad665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Q-values for different states\n",
    "def analyze_q_values(model, env, n_samples=100):\n",
    "    \"\"\"\n",
    "    Sample random states and analyze Q-values.\n",
    "\n",
    "    Args:\n",
    "        model: Trained DQN model\n",
    "        env: Gymnasium environment\n",
    "        n_samples: Number of states to sample\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        states, q_values, actions\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    q_values_left = []\n",
    "    q_values_right = []\n",
    "    chosen_actions = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        state, _ = env.reset()\n",
    "        states.append(state)\n",
    "\n",
    "        # Get Q-values for both actions\n",
    "        with torch.no_grad():\n",
    "            q_values = model.q_net(torch.FloatTensor(state).unsqueeze(0))\n",
    "            q_values_left.append(q_values[0, 0].item())\n",
    "            q_values_right.append(q_values[0, 1].item())\n",
    "            chosen_actions.append(torch.argmax(q_values).item())\n",
    "\n",
    "    return np.array(states), q_values_left, q_values_right, chosen_actions\n",
    "\n",
    "\n",
    "# Analyze Q-values\n",
    "states, q_left, q_right, actions = analyze_q_values(model, env, n_samples=300)\n",
    "\n",
    "print(\"\\n=== Q-Value Analysis ===\")\n",
    "print(f\"Average Q-value for LEFT: {np.mean(q_left):.2f}\")\n",
    "print(f\"Average Q-value for RIGHT: {np.mean(q_right):.2f}\")\n",
    "print(f\"Q-value range: [{min(q_left + q_right):.2f}, {max(q_left + q_right):.2f}]\")\n",
    "\n",
    "# Heatmap: Q-values based on Pole Angle and Cart Position\n",
    "# Create a grid of states to visualize Q-values\n",
    "print(\"\\nCreating Q-value heatmap...\")\n",
    "angle_range = np.linspace(-0.3, 0.3, 40)\n",
    "position_range = np.linspace(-2.4, 2.4, 40)\n",
    "q_grid_left = np.zeros((len(angle_range), len(position_range)))\n",
    "q_grid_right = np.zeros((len(angle_range), len(position_range)))\n",
    "\n",
    "for i, angle in enumerate(angle_range):\n",
    "    for j, position in enumerate(position_range):\n",
    "        # Create a state with this angle and position, zero velocities\n",
    "        test_state = np.array([position, 0.0, angle, 0.0])\n",
    "        with torch.no_grad():\n",
    "            q_values = model.q_net(torch.FloatTensor(test_state).unsqueeze(0))\n",
    "            q_grid_left[i, j] = q_values[0, 0].item()\n",
    "            q_grid_right[i, j] = q_values[0, 1].item()\n",
    "\n",
    "# Plot Q-value difference heatmap\n",
    "q_diff_grid = q_grid_right - q_grid_left\n",
    "\n",
    "fig = px.imshow(\n",
    "    q_diff_grid,\n",
    "    x=position_range,\n",
    "    y=angle_range,\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    color_continuous_midpoint=0,\n",
    "    title=\"Learned Policy: Q(RIGHT) - Q(LEFT) for Different States\",\n",
    "    labels={\"x\": \"Cart Position\", \"y\": \"Pole Angle (radians)\", \"color\": \"Q(RIGHT) - Q(LEFT)\"},\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Cart Position (negative = left of center, positive = right of center)\",\n",
    "    yaxis_title=\"Pole Angle (negative = leaning left, positive = leaning right)\",\n",
    "    height=500,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation of the Heatmap:\")\n",
    "print(\"- BLUE: Agent prefers action LEFT (push cart to the left)\")\n",
    "print(\"- RED: Agent prefers action RIGHT (push cart to the right)\")\n",
    "print(\"- The diagonal structure shows the learned strategy:\")\n",
    "print(\"  â†’ When pole leans left, push left\")\n",
    "print(\"  â†’ When pole leans right, push right\")\n",
    "print(\"- At the edges (extreme cart positions), the agent adjusts the strategy\")\n",
    "print(\"  to keep the cart within bounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b261e8",
   "metadata": {},
   "source": [
    "### Training met PPO (Proximal Policy Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d225019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create fresh environment\n",
    "env_ppo = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Create PPO model\n",
    "# PPO learns a policy Ï€(a|s) directly, not Q-values\n",
    "model_ppo = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_ppo,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train PPO\n",
    "model_ppo.learn(total_timesteps=50000, progress_bar=True)\n",
    "\n",
    "# Evaluate PPO (wrap env with Monitor to avoid warning)\n",
    "eval_env_ppo = Monitor(gym.make(\"CartPole-v1\"))\n",
    "mean_reward_ppo, std_reward_ppo = evaluate_policy(\n",
    "    model_ppo, eval_env_ppo, n_eval_episodes=100, deterministic=True\n",
    ")\n",
    "eval_env_ppo.close()\n",
    "\n",
    "print(f\"Random Agent: {np.mean(random_rewards):.2f} Â± {np.std(random_rewards):.2f}\")\n",
    "print(f\"DQN Agent:    {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "print(f\"PPO Agent:    {mean_reward_ppo:.2f} Â± {std_reward_ppo:.2f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "df_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Algorithm\": [\"Random\", \"DQN\", \"PPO\"],\n",
    "        \"Mean Reward\": [np.mean(random_rewards), mean_reward, mean_reward_ppo],\n",
    "        \"Std\": [np.std(random_rewards), std_reward, std_reward_ppo],\n",
    "    }\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_comparison,\n",
    "    x=\"Algorithm\",\n",
    "    y=\"Mean Reward\",\n",
    "    error_y=\"Std\",\n",
    "    title=\"Algorithm Performance Comparison on CartPole-v1\",\n",
    "    color=\"Algorithm\",\n",
    "    color_discrete_map={\"Random\": \"gray\", \"DQN\": \"blue\", \"PPO\": \"green\"},\n",
    "    text=[\n",
    "        f\"{m:.1f}Â±{s:.1f}\"\n",
    "        for m, s in zip(df_comparison[\"Mean Reward\"], df_comparison[\"Std\"], strict=False)\n",
    "    ],\n",
    ")\n",
    "fig.add_hline(y=500, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Maximum possible (500)\")\n",
    "fig.update_layout(yaxis_range=[0, 550])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4411a",
   "metadata": {},
   "source": [
    "## LunarLander\n",
    "\n",
    "Laten we nu een complexer probleem bekijken: **LunarLander-v2**. Hier moet een maanlander veilig landen op een landingsplatform.\n",
    "\n",
    "[![LunarLander](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/lunarLander.gif)](https://huggingface.co/learn/deep-rl-course)\n",
    "\n",
    "### Het Probleem\n",
    "- **State**: 8 continue waarden (positie, snelheid, hoek, hoeksnelheid, been-contact)\n",
    "- **Actions**: 4 discrete acties (niets, linker motor, hoofd motor, rechter motor)\n",
    "- **Rewards**: \n",
    "  - +100 tot +140 voor succesvolle landing\n",
    "  - -100 voor crash\n",
    "  - Kleine negatieve rewards voor brandstofverbruik\n",
    "  - Positieve rewards voor dichter bij landingszone\n",
    "- **Doel**: Land veilig met minimaal brandstofverbruik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ababe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LunarLander environment\n",
    "env_lunar = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Explore the environment\n",
    "state, info = env_lunar.reset(seed=42)\n",
    "\n",
    "print(\"=== LunarLander-v2 Environment ===\")\n",
    "print(f\"State space: {env_lunar.observation_space}\")\n",
    "print(f\"Action space: {env_lunar.action_space}\")\n",
    "print(f\"\\nInitial state shape: {state.shape}\")\n",
    "print(f\"State: {state}\")\n",
    "print(\"\\nState components:\")\n",
    "print(\"  [0] X position\")\n",
    "print(\"  [1] Y position\")\n",
    "print(\"  [2] X velocity\")\n",
    "print(\"  [3] Y velocity\")\n",
    "print(\"  [4] Angle\")\n",
    "print(\"  [5] Angular velocity\")\n",
    "print(\"  [6] Left leg contact (0=no, 1=yes)\")\n",
    "print(\"  [7] Right leg contact (0=no, 1=yes)\")\n",
    "print(\"\\nActions:\")\n",
    "print(\"  0: Do nothing\")\n",
    "print(\"  1: Fire left engine\")\n",
    "print(\"  2: Fire main engine\")\n",
    "print(\"  3: Fire right engine\")\n",
    "\n",
    "# Test random agent on LunarLander\n",
    "print(\"\\n=== Testing Random Agent ===\")\n",
    "random_rewards_lunar = evaluate_random_agent(env_lunar, n_episodes=20, seed=42)\n",
    "print(f\"Random Agent: {np.mean(random_rewards_lunar):.2f} Â± {np.std(random_rewards_lunar):.2f}\")\n",
    "print(\"(Note: Negative rewards mean crashes!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52651e2",
   "metadata": {},
   "source": [
    "### Training met PPO op LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO on LunarLander\n",
    "model_lunar = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_lunar,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    clip_range=0.2,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Train for longer since this is more complex\n",
    "model_lunar.learn(total_timesteps=500000, progress_bar=True)\n",
    "\n",
    "# Evaluate (wrap env with Monitor to avoid warning)\n",
    "eval_env_lunar = Monitor(gym.make(\"LunarLander-v3\"))\n",
    "mean_reward_lunar, std_reward_lunar = evaluate_policy(\n",
    "    model_lunar, eval_env_lunar, n_eval_episodes=50, deterministic=True\n",
    ")\n",
    "eval_env_lunar.close()\n",
    "\n",
    "print(f\"Random Agent: {np.mean(random_rewards_lunar):.2f} Â± {np.std(random_rewards_lunar):.2f}\")\n",
    "print(f\"Trained PPO:  {mean_reward_lunar:.2f} Â± {std_reward_lunar:.2f}\")\n",
    "print(\"\\nNote: Score > 200 is considered solved!\")\n",
    "status = \"SOLVED âœ“\" if mean_reward_lunar > 200 else \"Needs more training\"\n",
    "print(f\"Status: {status}\")\n",
    "\n",
    "# Visualize performance\n",
    "df_lunar = pd.DataFrame(\n",
    "    {\n",
    "        \"Algorithm\": [\"Random\", \"PPO\"],\n",
    "        \"Mean Reward\": [np.mean(random_rewards_lunar), mean_reward_lunar],\n",
    "        \"Std\": [np.std(random_rewards_lunar), std_reward_lunar],\n",
    "    }\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_lunar,\n",
    "    x=\"Algorithm\",\n",
    "    y=\"Mean Reward\",\n",
    "    error_y=\"Std\",\n",
    "    title=\"LunarLander-v2 Performance\",\n",
    "    color=\"Algorithm\",\n",
    "    color_discrete_map={\"Random\": \"gray\", \"PPO\": \"green\"},\n",
    ")\n",
    "fig.add_hline(y=200, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Solved threshold (200)\")\n",
    "fig.add_hline(y=0, line_color=\"black\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598f833",
   "metadata": {},
   "source": [
    "### Visualisatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trained LunarLander agent\n",
    "env_lunar_render = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "frames_lunar, rewards_lunar, actions_lunar = visualize_agent(\n",
    "    model_lunar, env_lunar_render, n_steps=500\n",
    ")\n",
    "env_lunar_render.close()\n",
    "\n",
    "print(\"\\n=== Episode Analysis ===\")\n",
    "print(f\"Episode length: {len(rewards_lunar)} steps\")\n",
    "print(f\"Total reward: {sum(rewards_lunar):.1f}\")\n",
    "print(\n",
    "    f\"Final outcome: {'SUCCESS âœ“' if sum(rewards_lunar) > 200 else 'CRASH' if sum(rewards_lunar) < 0 else 'PARTIAL'}\"\n",
    ")\n",
    "print(\"\\nAction usage:\")\n",
    "action_names = [\"Do nothing\", \"Left engine\", \"Main engine\", \"Right engine\"]\n",
    "for action_id, action_name in enumerate(action_names):\n",
    "    count = actions_lunar.count(action_id)\n",
    "    percentage = (count / len(actions_lunar)) * 100\n",
    "    print(f\"  {action_name}: {count} times ({percentage:.1f}%)\")\n",
    "\n",
    "# Show action distribution\n",
    "df_actions = pd.DataFrame(\n",
    "    {\"Action\": action_names, \"Count\": [actions_lunar.count(i) for i in range(4)]}\n",
    ")\n",
    "px.bar(\n",
    "    df_actions,\n",
    "    x=\"Action\",\n",
    "    y=\"Count\",\n",
    "    title=f\"LunarLander Action Distribution (Total Reward: {sum(rewards_lunar):.1f})\",\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation of the trained LunarLander agent\n",
    "print(\"Creating LunarLander animation...\")\n",
    "anim_lunar = create_animation(frames_lunar, interval=50)\n",
    "\n",
    "# Display the animation\n",
    "HTML(anim_lunar.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
