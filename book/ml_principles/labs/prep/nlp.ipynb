{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb5222d",
   "metadata": {},
   "source": [
    "# Vragen\n",
    "\n",
    "## [Input notebook](./sentiment_imdb.ipynb)\n",
    "\n",
    "## Algemene instructies\n",
    "- Je geeft je antwoord telkens na de âœï¸. Na de ğŸ”‘ kan je _beschrijven_ hoe je tot dit antwoord kwam (gebruik de cel nummers in de input notebook ter referentie).\n",
    "- Er wordt enkel naar de beschrijving bij ğŸ”‘ gekeken indien je antwoord bij âœï¸ fout is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5140a5c",
   "metadata": {},
   "source": [
    "### Q1\n",
    "â“ Met welke soort taak hebben we hier te maken?\n",
    "  \n",
    "âœï¸  \n",
    "Klassificatie van tekst (sentiment analyse)\n",
    "  \n",
    "ğŸ”‘  \n",
    "\n",
    "De target variabele heeft 2 niveaus die overeenkomen met positieve en negatieve recensies.\n",
    "\n",
    "```\n",
    "classes = [\"Negative\", \"Positive\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0dc24",
   "metadata": {},
   "source": [
    "### Q2\n",
    "â“ Met welke soort van model wordt hier gewerkt?\n",
    "  \n",
    "âœï¸  \n",
    "Een Transformer model (DistilBERT)\n",
    "\n",
    "ğŸ”‘  \n",
    "\n",
    "Zie `model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)` en de output van `print(model)`. Dit toont dat er een transformer blok aanwezig is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643827c4",
   "metadata": {},
   "source": [
    "### Q3\n",
    "â“ Hoeveel parameters heeft het model?\n",
    "  \n",
    "âœï¸  \n",
    "66955010\n",
    "  \n",
    "ğŸ”‘  \n",
    "\n",
    "Wordt getoond in de output van cel 10 waar het model wordt geladen: `Total number of trainable parameters: 66,955,010`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b18f0",
   "metadata": {},
   "source": [
    "### Q4\n",
    "â“ Met welke soort ervaring wordt er hier geleerd?\n",
    "  \n",
    "âœï¸  \n",
    "Supervised learning (fine-tuning)\n",
    "  \n",
    "ğŸ”‘  \n",
    "\n",
    "Er zijn target waarden (labels) aanwezig in de dataset die aangeven of een recensie positief of negatief is. Deze worden gebruikt om de loss te berekenen tijdens het fine-tunen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095c52c",
   "metadata": {},
   "source": [
    "### Q5\n",
    "â“ Na hoeveel inputs worden gewichten aangepast tijdens de training?\n",
    "  \n",
    "âœï¸  \n",
    "16\n",
    "  \n",
    "ğŸ”‘  \n",
    "\n",
    "Dit is de batch size: zie `batch_size = 16`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758ece9",
   "metadata": {},
   "source": [
    "### Q6\n",
    "â“ Hoe vaak heeft het model de trainingsdata gezien tijdens het trainen?\n",
    "  \n",
    "âœï¸  \n",
    "2\n",
    "  \n",
    "ğŸ”‘  \n",
    "\n",
    "Dit is het aantal epochs: zie `epochs = 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2bd707",
   "metadata": {},
   "source": [
    "### Q7\n",
    "â“ Welke verliesfunctie wordt hier gebruikt?\n",
    "  \n",
    "âœï¸  \n",
    "Cross Entropy Loss\n",
    "  \n",
    "ğŸ”‘  \n",
    "\n",
    "De loss wordt automatisch berekend door het model zelf wanneer we labels meegeven: `Loss function (CrossEntropyLoss is used internally by the model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd18283",
   "metadata": {},
   "source": [
    "### Q8\n",
    "â“ Welk optimalisatie algoritme wordt hier gebruikt?\n",
    "  \n",
    "âœï¸  \n",
    "AdamW (Adam met weight decay)\n",
    "  \n",
    "ğŸ”‘  \n",
    "\n",
    "Zie `optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff288b33",
   "metadata": {},
   "source": [
    "### Q9\n",
    "â“ Met welke stapgrootte wordt er tijdens de optimalisatie gewerkt?\n",
    "  \n",
    "âœï¸  \n",
    "2e-5\n",
    "\n",
    "ğŸ”‘  \n",
    "\n",
    "Zie `learning_rate = 2e-5` en `optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)`. Deze kleine learning rate is typisch voor het fine-tunen van pre-trained transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c82af",
   "metadata": {},
   "source": [
    "### Q10\n",
    "â“ Welke score metrics worden hier bekeken?\n",
    "  \n",
    "âœï¸  \n",
    "Accuracy  \n",
    "Precision  \n",
    "Recall  \n",
    "F1-score\n",
    "\n",
    "ğŸ”‘  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721f99a",
   "metadata": {},
   "source": [
    "### Q11\n",
    "â“ Hoe worden de text inputs aan het model doorgegeven?\n",
    "  \n",
    "âœï¸  \n",
    "Via een tokenizer.\n",
    "\n",
    "ğŸ”‘  \n",
    "zie `tokenizer = AutoTokenizer.from_pretrained(model_name)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a0962",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
