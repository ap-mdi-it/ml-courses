{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d5c62c",
   "metadata": {},
   "source": [
    "#  üéÆ Actieplanning\n",
    "üìö [^hf]\n",
    "[^hf]: Deze sectie is gebaseerd op de [Hugging Face Deep RL Course](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction).\n",
    "\n",
    ":::{iframe} https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/huggy.mp4\n",
    ":width: 100%\n",
    ":::\n",
    "\n",
    "## _Reinforcement Learning_\n",
    "\n",
    "Zoals we reeds zagen in de bespreking van [ML taken](../basics/tasks.ipynb), is het bij {ref}`actieplanning <target-planning>` de bedoeling om optimale actiesequenties te leren, met het oog op het bereiken van een bepaald doel. Hierbij wordt gewerkt met een specifieke vorm van [ervaring](../basics/experience.ipynb): _{ref}`reinforcement learning <target-reinforcement-learning>`_ (RL). In tegenstelling tot _(self-)supervised learning_, wordt er bij RL in de zoektocht naar optimale parameters **geen rechtstreekse feedback** gegeven over de juistheid van een individuele actie. In tegenstelling tot _unsupervised learning_, wordt het leren gestuurd door op zoek te gaan naar een **zo groot mogelijke cumulatieve beloning uit de omgeving**.\n",
    "\n",
    ":::{important}\n",
    "(target-rl-agent)=\n",
    "RL is dus een specifiek principe waarmee een model [ervaring](../basics/experience.ipynb) kan opdoen dat vooral gebruikt wordt bij {ref}`actieplanning <target-planning>`. principe. **RL verwijst dus {u}`niet` naar een specifiek {ref}`model <target-model>`**! Zo zagen we reinforcement al onder de vorm van _reinforcement learning from human feedback_ (RLHF) in de context van het finetunen van large language modellen. In deze secties bespreken we de benadering specifiek in het kader van {ref}`actieplanning <target-planning>`. Daarbij wordt vaak de term **\"_agent_\"** gebruikt. Dit verwijst algemeen naar het systeem dat **iteratief observeert, acties neemt en resultaten van acties integreert**. De agent kan daarbij rekenen op de voorspellingen van **√©√©n of meerdere ML modellen**.\n",
    ":::\n",
    "\n",
    "_Agents_ die via RL getraind worden, hebben de volgende algemene eigenschappen:\n",
    "- **Iteratieve output**: De _agent_ bepaalt niet in √©√©n keer de hele sequentie van acties, maar registreert en integreert na iedere actie feedback over de gevolgen van die actie in de omgeving in een nieuwe (voorspelde) actie.\n",
    "- **Trial-and-error learning**: Leren gebeurt door acties uit te proberen en de uiteindelijke beloning te observeren.\n",
    "- **Markov-eigenschap**: Dit is een centrale aanname: om te weten wat de beste volgende actie is, heeft de _agent_ enkel kennis nodig van de huidige stand van zaken in de omgeving (_state_). In een schaakspel, bijvoorbeeld, worden acties enkel genomen op basis van reflecties over de toekomst, gegeven de huidige staat op het bord, niet op basis van reflecties over hoe de huidige staat tot stand kwam.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c13e7",
   "metadata": {},
   "source": [
    "## RL Loop\n",
    "(target-rl-loop)=\n",
    "Het RL proces moet gezien worden als een continue loop van interacties tussen een _agent_ en een omgeving:\n",
    "\n",
    "[![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg)](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
    "\n",
    "De cyclus werkt als volgt:\n",
    "1. De _agent_ observeert de huidige _state_ $S_t$ van de omgeving\n",
    "2. Op basis van die state kiest de agent een actie $A_t$\n",
    "3. Dit geeft aanleiding tot een een nieuwe _state_ $S_{t+1}$ in de omgeving\n",
    "4. Of basis van die verandering, krijgt de _agent_ een _reward_ $R_{t+1}$ signaal uit de omgeving\n",
    "\n",
    "**Het doel van de agent is om de _cumulatieve reward_ over tijd te maximaliseren**.\n",
    "\n",
    "[![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg)](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
    "\n",
    "## Kernconcepten\n",
    "### _Omgeving_\n",
    "(target-rl-env)=\n",
    "Dit is de \"wereld\" waarin de _agent_ zich begeeft.\n",
    "\n",
    "### _States_\n",
    "(target-rl-state)=\n",
    "Een **_state_** is een momentane beschrijving van de omgeving.\n",
    "De _state_ is hetzij {u}`volledig geobserveerd` (bv. bij een schaakbord),\n",
    "\n",
    "[![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/chess.jpg)](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
    "\n",
    "hetzij {u}`gedeeltelijk geobserveerd` (bv. bij een videogame zoals Super Mario).\n",
    "\n",
    "[![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg)](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction)\n",
    "\n",
    "In de praktijk onderscheiden we daarom:\n",
    "- **_State_** $s_t$: De complete beschrijving van de omgeving op een bepaald moment $t$.\n",
    "(target-rl-observation)=\n",
    "- **Observatie** $o_t$: Het gedeelte van de omgeving dat de agent waarneemt op moment $t$.\n",
    "\n",
    "### Acties\n",
    "(target-rl-action-space)=\n",
    "De **_action space_** bevat alle mogelijke acties die een agent kan uitvoeren:\n",
    "- Discrete _action space_: Eindig aantal acties (bijv. Super Mario: links, rechts, springen, bukken)\n",
    "- Continue _action space_: Oneindig aantal acties (bijv. de stuurhoek bij een zelfrijdende wagen)\n",
    "\n",
    "### _Rewards_\n",
    "(target-rl-reward)=\n",
    "De **_reward_** of beloning is het enige feedbacksignaal voor de agent waaruit het tijdens de training de optimale parameters gezocht kunnen worden voor de onderliggende ML modellen. De _reward_ hypothese stelt dat alle doelen kunnen worden beschreven als het maximaliseren van de verwachte cumulatieve _reward_.\n",
    "\n",
    "(target-rl-return)=\n",
    "De **cumulatieve _reward_** of **_return_** is gegeven als:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "R(\\tau) &= r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4}\\ldots \\cr\n",
    "&= \\sum_{k=0}^{\\infty} r_{t+k+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "waarbij $\\tau$ staat voor het traject: de sequentie van _state_-actieparen tot op moment $t$.\n",
    "\n",
    "(target-rl-discounting)=\n",
    "Op basis van de onderliggende ML modellen, kunnen _agents_ (expliciet of impliciet) rekening houden met _expected_ cumulatieve _rewards_ om de meest wenselijke _volgende_ actie te bepalen. Omdat voorspellingen onzekerder zijn naarmate ze verder in de toekomst liggen, wordt bij de berekening van de _return_ een correctie toegepast naarmate de returns verder in de toekomst liggen. Hierdoor kan er bij de keuzes van acties voorkeur gegeven worden aan acties met een meer onmiddellijke winst. Dit gebeurt aan de hand van een **_discount_ factor**, wat resulteert in de **_discounted return_** is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "R(\\tau) &= r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 r_{t+4} + \\ldots \\cr\n",
    "&= \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "waarbij $\\gamma$ (gamma) de **_discount_ factor** is ($0 \\leq \\gamma \\leq 1$):\n",
    "- $\\gamma$ dicht bij 1: Agent houdt rekening met lange-termijn rewards\n",
    "- $\\gamma$ dicht bij 0: Agent focust op korte-termijn rewards\n",
    "\n",
    "### _Value_ functie\n",
    "(target-rl-value-function)=\n",
    "De _value_ functie is de functie die de verwachte _discounted return_ bepaalt gegeven een bepaalde _state_ ($V$-functie) of _state_-actiepaar ($Q$-functie).\n",
    "\n",
    "### Policy\n",
    "(target-rl-policy)=\n",
    "De **policy** $\\pi$ is de strategie van de agent - het bepaalt welke actie de agent kiest in een bepaalde state:\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = P(A_t = a | S_t = s)\n",
    "$$\n",
    "\n",
    "Dit kan zijn:\n",
    "- Deterministisch: $a = \\pi(s)$ - altijd dezelfde actie voor een state\n",
    "- Stochastisch: $\\pi(a|s)$ - kansverdeling over acties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd978e7",
   "metadata": {},
   "source": [
    "## Modellen\n",
    "\n",
    "Het **doel van de agent is om tijdens de trainingsfase de optimale policy** $\\pi^*$ te vinden die de verwachte return maximaliseert. Dit kan op twee manieren:\n",
    "\n",
    "(target-rl-policy-based)=\n",
    "(target-rl-value-based)=\n",
    "1. **_Policy-based_**: Er wordt een ML model getraind met _states_ als input en acties als output. Daaruit krijgt de _agent_ bij _inference_ dus rechtstreeks informatie over de optimale actie.\n",
    "2. **_Value based_**: Een ML model leert de _return_ te voorspellen, voor verschillende _states_ of _state_-actieparen. Daarna kan de _agent_, gegeven een bepaalde _state_, bijvoorbeeld de actie kiezen met de grootste verwachte _return_ (dit is een zogenaamde _greedy policy_).\n",
    "\n",
    "In principe kan ieder type ML model hierin gekoppeld worden. In wat volgt zullen we echter focussen op toepassingen met _deep learning_ modellen of zogenaamde _Deep RL_. De beschrijving van deze modellen, in termen van architectuur, parameters, features en hyperparameters wordt bepaald door de use-case en heeft geen specifieke eigenschappen die gekoppeld zijn an het domein van actieplanning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b0996",
   "metadata": {},
   "source": [
    "## Leeralgoritmes\n",
    "\n",
    "De training is bij RL doorgaans veel complexer dan bij andere domeinen van ML. Dat komt vooral door de verschillende leeralgoritmes, hyperparameters (bv. om exploratie versus exploitatie te bepalen) en complexe _loss_ functies om de _reward_ signalen te vertalen naar de updates voor de modelparameters. Door de grote **variabiliteit**, bestaat er een re√´el **risico op instabiele training**. Die variabiliteit komt uiteraard ook uit de omgeving van de agent zelf die soms heel complex kunnen zijn (denk bv. aan een zelfrijdende wagen). Hoe krachtig de resultaten van RL ook kunnen zijn, het toepassen van RL in een nieuwe omgeving blijft doorgaans heel uitdagend.  \n",
    "  \n",
    "De leeralgoritmes hangen nauw samen met de gekozen modeltypes: _policy based_, _value based_ of beide.\n",
    "\n",
    "### _Q-learning_\n",
    "(target-rl-q-learning)=\n",
    "_**Q-Learning**_ is een _value based_ algoritme waarbij een optimale _state_-actie _value_ functie (of model) $Q^*(s, a)$ geleerd wordt. Een _greedy policy_ neemt dan meestal de volgende vorm aan:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\underset{a}{\\operatorname{\\argmax}} Q^*(s, a)\n",
    "$$\n",
    "\n",
    ":::{note} Exploratie versus exploitatie\n",
    "Tijdens het trainen kan een agent gestuurd worden richting meer of minder exploratie van de $Q$ ruimte. Bij een _greedy policy_ kiest de _agent_ telkens voor de actie met de hoogste verwachte (_discounted_) _return_ (\"exploitatie\"). Daardoor kan het leertraject traag verlopen omdat de agent niet of te weinig terecht komt in onbekende states die mogelijks tot een hogere _return_ leiden. Daarom kiest men meestal voor een zogenaamde $\\epsilon$-_greedy policy_ waarbij met een probabiliteit $\\epsilon$ een random actie (exploration) genomen wordt in plaats van de beste actie volgens $Q(s, a)$\n",
    ":::\n",
    "\n",
    "Bij _Deep Q Learning_ wordt de zogenaamde $Q$-_loss_ berekend door de voorspelde (_discounted_) return bij een gegeven _state_-actionpaar te vergelijken met een $Q$-_target_ (in de standaard MSE vorm). Die _target_ wordt {u}`geschat` door de voorspellingen van het model zelf (met parameters $\\theta$) over de $Q$-_value_ van de volgende state $s'$, samen met de onmiddellijke _return_ $r$, in de zogenaamde {u}`_Bellman_ vergelijking` in te vullen:\n",
    "\n",
    "$$\n",
    "Q_{\\text{target}} = r + \\gamma \\underset{a'}{\\operatorname{\\argmax}} Q(s', a', \\theta)\n",
    "$$\n",
    "\n",
    "De gradient van de $Q$-_loss_ wordt dan gebruikt om de parameters van het neurale netwerk via standaard _backpropagation_ en _gradient descent_ te updaten.\n",
    "\n",
    "### _Policy gradient learning_\n",
    "(target-rl-policy-gradient)=\n",
    "Deze familie van leeralgoritmes zijn _policy based_. Er wordt dus rechtstreeks een optimale _policy_-functie (of model) ${\\pi_{\\theta}}^*(a|s)$ geleerd. **_Policy gradient learning_** past _gradient ascent_ toe op de gradient van de verwachtte _return_ (om die dus te maximaliseren):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla J(\\theta) &\\approx \\mathbb{E}\\left[ \\nabla \\log \\pi(a|s; \\theta) \\cdot G_t \\right] \\cr\n",
    "\\pmb{\\theta}^{k+1} &= \\pmb{\\theta}^k + \\lambda \\nabla J(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Waarbij:\n",
    "- $\\nabla \\log \\pi(a|s; \\theta)$: De richting die de probabiliteit van actie $a$ verhoogt.\n",
    "- $G_t$: De _return_ (cumulatieve _reward_) vanaf tijdstip $t$.\n",
    "- Het dot-product: Verhoogt de probabiliteit van acties die tot hoge _returns_ leidden en verlaag de probabiliteit van acties tot lage _returns_ leidden.\n",
    "  \n",
    "Dit vormt de basis van het REINFORCE algoritme door {cite}`10.1007/BF00992696`. In recent werk wordt meestal gewerkt met het zogenaamde _{u}`Proximal Policy Optimization`_ (PPO) algoritme wat over het algemeen stabielere resultaten oplevert.  \n",
    "  \n",
    "Merk op dat de optimalisatie hier niet op basis van een _loss_ functie gebeurt, maar door rechtstreeks de verwachtte _return_ $J(\\theta)$ te maximaliseren[^ascent].\n",
    "\n",
    "[^ascent]: Vandaar dat we hier te maken hebben met _gradient ascent_ en niet _gradient descent_.\n",
    "\n",
    "### _Actor-critic learning_\n",
    "(target-rl-actor-critic)=\n",
    "Hierbij wordt een _value_-functie gecombineerd met een _policy_-functie. De _actor_ leert de _policy_ ${\\pi_{\\theta}}^*(a|s)$ terwijl de _critic_ de _value_-functie $Q^*(s, a)$ of $V^*(s)$ leert. In plaats van de werkelijke _returns_ $G_t$ te gebruiken bij het updaten van de policy (zoals bij _policy gradient learning_), worden _value_-predicties van de _critic_ gebruikt (_expected return_). Net zoals het PPO algoritme, leidt deze techniek over het algemeen tot stabielere resultaten. Doordat er niet moet gewacht worden tot tot het einde van een interactie om de werkelijke _return_ te berekenen, wordt deze benadering vaak gekozen omwille van _sample_ effici√´ntie en bij continue taken (bv. robotica; zie verder). De meest populaire variant _{u}`Soft-Actor-Critic`_ (SAC) introduceert ook entropie in de parameter-optimalisatie functie ({cite}`10.48550/arXiv.1801.01290`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab49db4",
   "metadata": {},
   "source": [
    "## Taken\n",
    "We maken een onderscheid tussen twee soorten taken afhankelijk of er sprake is van een eind-_state_.\n",
    "\n",
    "### Episodische taken\n",
    "Hierbij is er een duidelijke eind-_state_, bijvoorbeeld, wanneer een game uitgespeeld is.\n",
    "### Continue taken\n",
    "Hierbij is er geen eindpunt. Denk aan autonoom rijden, robotica, _trading_, enz.\n",
    "\n",
    "[![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/stock.jpg)](https://huggingface.co/learn/deep-rl-course/en/unit1/tasks)\n",
    "\n",
    "RL is geschikt voor problemen waarbij:\n",
    "- sequenti√´le beslissingen cruciaal zijn,\n",
    "- interactie met omgeving mogelijk is,\n",
    "- feedback of wenselijk of onwenselijke predicties (_rewards_) pas na verloop van tijd duidelijk worden en\n",
    "- geen expliciete labels beschikbaar zijn,\n",
    "\n",
    "zoals bij:\n",
    "- üéÆ Game AI: AlphaGo, Dota 2, StarCraft II\n",
    "- ü§ñ Robotica: Lopen, grijpen, manipulatie\n",
    "- üöó Autonome systemen: Zelfrijdende auto's\n",
    "- üí∞ Trading: Automatische handelssystemen\n",
    "- üè≠ Resource management: Datacenter koeling, energie-optimalisatie\n",
    "- üíä Gezondheidszorg: Behandelprotocollen, medicatie timing\n",
    "- üéØ Aanbevelingssystemen: Personalisatie over de tijd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fc054",
   "metadata": {},
   "source": [
    "## Evaluatie\n",
    "Bij RL evalueren we op basis van scores zoals:\n",
    "- _Average return_: De gemiddelde cumulatieve _reward_ over meerdere episodes.\n",
    "- _Success rate_: Het percentage van de episodes waarin een specifiek doel wordt bereikt.\n",
    "- Leercurve: Hoe snel verbetert de agent tijdens training?\n",
    "- enz.  \n",
    " \n",
    "Dit is anders dan typische scores zoals accuracy bij klassificatie of MSE bij regressie, omdat we ge√Ønteresseerd zijn in de {u}`prestaties over de tijd`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7edfb8a",
   "metadata": {},
   "source": [
    "\n",
    "## Voordelen\n",
    "- Geen gelabelde data nodig: RL leert rechtstreeks uit interactie met de omgeving, zonder vooraf gelabelde voorbeelden\n",
    "- Sequenti√´le besluitvorming: Ideaal voor problemen waar acties gevolgen hebben op toekomstige situaties\n",
    "- Adaptief leren: Agents kunnen zich aanpassen aan veranderende omgevingen en nieuwe situaties\n",
    "- Autonomie: Agents ontdekken zelf strategie√´n die mensen misschien niet hadden bedacht (bv. AlphaGo)\n",
    "- Breed toepassingsgebied: Van games en robotica tot resource management en gezondheidszorg\n",
    "- Trial-and-error: Natuurlijke leerwijze die exploratie en experimenteren mogelijk maakt\n",
    "\n",
    "## Nadelen\n",
    "- Sample ineffici√´ntie: Vaak miljoenen interacties nodig, zeker bij complexe omgevingen\n",
    "- _Reward_ design: Vaak moeilijk om een goede reward functie te defini√´ren\n",
    "- Exploitatie-Exploratie trade-off\n",
    "- _Credit assignment_: Uitdagend om wiskundig te bepalen welke acties verantwoordelijk waren voor _rewards_ (of het gebrek daaraan)\n",
    "- Training stability: RL training kan heel instabiel zijn\n",
    "- _Sparse rewards_: Bij veel problemen krijg je zelden feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
